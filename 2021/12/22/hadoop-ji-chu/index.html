<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="hadoop基础, tangyuan&#39;s blog">
    <meta name="description" content="hadoop1 初始hadoop1.1 1T文件的操作思考
   分治思想
   单机处理大数据的问题
   集群分布式处理大数据的辩证

1.1.1    分治思想引入案例
十万个元素（数字或单词）需要存储，如何存储？单一遍历复杂度
  ">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>hadoop基础 | tangyuan&#39;s blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">tangyuan&#39;s blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">tangyuan&#39;s blog</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/tangyuan59/" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/tangyuan59/" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">hadoop基础</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="container content">

    
    <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Hadoop/">
                                <span class="chip bg-color">Hadoop</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                大数据
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-12-22
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    87 分
                </div>
                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><h3 id="1-初始hadoop"><a href="#1-初始hadoop" class="headerlink" title="1 初始hadoop"></a>1 初始hadoop</h3><h4 id="1-1-1T文件的操作思考"><a href="#1-1-1T文件的操作思考" class="headerlink" title="1.1 1T文件的操作思考"></a>1.1 1T文件的操作思考</h4><ol>
<li>   分治思想</li>
<li>   单机处理大数据的问题</li>
<li>   集群分布式处理大数据的辩证</li>
</ol>
<h5 id="1-1-1-分治思想引入案例"><a href="#1-1-1-分治思想引入案例" class="headerlink" title="1.1.1    分治思想引入案例"></a>1.1.1    分治思想引入案例</h5><ol>
<li>十万个元素（数字或单词）需要存储，如何存储？<br><a target="_blank" rel="noopener" href="https://ibb.co/cxGhwqs">单一遍历复杂度</a></li>
<li>   如果想查找某一个元素，最简单的遍历方式的复杂度是多少？<br><a target="_blank" rel="noopener" href="https://ibb.co/pKxmyKP">多遍历复杂度</a></li>
</ol>
<p>•    分而治之的思想非常重要，常见于以下技术：<br>    1.    Redis集群<br>    2.    Hadoop<br>    3.    Hbase<br>    4.    ElasticSearch</p>
<h5 id="1-1-2-单机处理大数据的问题"><a href="#1-1-2-单机处理大数据的问题" class="headerlink" title="1.1.2    单机处理大数据的问题"></a>1.1.2    单机处理大数据的问题</h5><p>需求：<br>    有一个非常大的文本文件，里面有非常多的行，只有两行内容一样，它们出现在未知的位置，需要查找到它<br>•    假如IO速度是500MB/S<br>•    1T文件读取一遍需要约30分钟<br>•    循环遍历需要N次IO时间<br><a target="_blank" rel="noopener" href="https://ibb.co/WHkqWNj">图例</a></p>
<p>•    分治思想可以使时间降为2次IO<br>•    思考：<br>    •    如何让时间变为分钟、秒级别</p>
<p>•    假如IO速度是500MB/S<br>•    1T文件读取一遍需要约30分钟<br>•    如何对1TB文件进行排序<br><a target="_blank" rel="noopener" href="https://ibb.co/fYqk13G">图例</a></p>
<p>方式1：外部有序，内部无序。 逐一读入内存排序<br>方式2：逐一读取500M排序，内部有序，    外部无序 ，然后进行归并排序</p>
<p>需求：<br>•    有一个非常大的文本文件，里面有几百亿行，只有两行内容一样，它们出现在未知的位置，需要查找到它们。<br>•    分钟、秒级别完成<br>•    硬件：*台机器，而且可用的内存500MB</p>
<p>由于涉及到计算机之间文件传输，千兆带宽，100MB/s<br>拉取网卡100MB/S<br><a target="_blank" rel="noopener" href="https://ibb.co/T2BTCSW">图例</a><br>之前忽略了上传时间：1TB/100MB = 10000S     /3600S   = 3H</p>
<h5 id="1-1-3-集群分布式处理大数据的辩证"><a href="#1-1-3-集群分布式处理大数据的辩证" class="headerlink" title="1.1.3    集群分布式处理大数据的辩证"></a>1.1.3    集群分布式处理大数据的辩证</h5><p>•    2000台真的比一台快吗？<br>•    如果考虑分发上传文件的时间呢？<br>•    如果考虑每天都有1TB数据的产生呢？<br>•    如果增量了一年，最后一天计算数据呢？<br>•    1 天   2*30=1H              3H1M2S<br>•    2天    2H                   3H1M4S<br>•    3天    3H                   3H1M6S<br>•    4天    4H                   3H1M8S</p>
<h4 id="1-2-hadoop起源"><a href="#1-2-hadoop起源" class="headerlink" title="1.2    hadoop起源"></a>1.2    hadoop起源</h4><h5 id="1-2-1-发展历史"><a href="#1-2-1-发展历史" class="headerlink" title="1.2.1    发展历史"></a>1.2.1    发展历史</h5><blockquote>
</blockquote>
<h5 id="1-2-2-核心组件"><a href="#1-2-2-核心组件" class="headerlink" title="1.2.2    核心组件"></a>1.2.2    核心组件</h5><ol>
<li>   hadoop通用组件 - Hadoop Common包含了其他hadoop模块要用到的库文件和工具</li>
<li>   分布式文件系统 - Hadoop Distributed File System (HDFS) 运行于通用硬件上的分布式文件系统，高吞吐，高可靠</li>
<li>   资源管理组件 - Hadoop YARN于2012年引入的组件，用于管理集群中的计算资源并在这些资源上调度用户应用。</li>
<li>   分布式计算框架 - Hadoop MapReduce 用于处理超大数据集计算的MapReduce编程模型的实现。</li>
<li>   Hadoop Ozone: An object store for Hadoop.</li>
<li>   Hadoop Submarine: A machine learning engine for Hadoop</li>
</ol>
<h5 id="1-2-3-hadoop关联项目"><a href="#1-2-3-hadoop关联项目" class="headerlink" title="1.2.3    hadoop关联项目"></a>1.2.3    hadoop关联项目</h5><ol>
<li>   Apache Ambari是一种基于Web的工具，支持Apache Hadoop集群的供应、管理和监控。Apache Ambari 支持HDFS、MapReduce、Hive、Pig、Hbase、Zookeepr、Sqoop和Hcatalog等的集中管理。也是5个顶级hadoop管理工具之一。</li>
<li>   Avro™:数据序列化系统</li>
<li>   Cassandra是一套开源分布式NoSQL数据库系统。它最初由Facebook开发，用于储存收件箱等简单格式数据，集GoogleBigTable的数据模型与Amazon Dynamo的完全分布式的架构于一身，Facebook于2008将 Cassandra 开源。</li>
<li>   chukwa 是一个开源的用于监控大型分布式系统的数据收集系统。这是构建在 hadoop 的 HDFS 和MapReduce框架之上的，继承了 hadoop 的可伸缩性和健壮性。Chukwa 还包含了一个强大和灵活的工具集，可用于展示、监控和分析已收集的数据。</li>
<li>   hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。</li>
<li>   Mahout 提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。</li>
<li>   Apache Pig 是一个高级过程语言，适合于使用 Hadoop 和 MapReduce 平台来查询大型半结构化数据集。通过允许对分布式数据集进行类似 SQL 的查询，Pig 可以简化 Hadoop 的使用。</li>
<li>   Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是UC Berkeley AMP lab开源的类Hadoop MapReduce的通用并行框架，拥有MapReduce所具有的优点；但是Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。</li>
<li>   Tez 是 Apache 最新的支持 DAG 作业的开源计算框架。它允许开发者为最终用户构建性能更快、扩展性更好的应用程序。Hadoop传统上是一个大量数据批处理平台。但是，有很多用例需要近乎实时的查询处理性能。还有一些工作则不太适合MapReduce，例如机器学习。Tez的目的就是帮助Hadoop处理这些用例场景。</li>
<li>   ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</li>
<li>   HBase是一个分布式的、高可靠性、高性能、面向列、可伸缩的分布式存储系统，该技术来源于Fay Chang所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力</li>
</ol>
<h4 id="1-3-HDFS架构"><a href="#1-3-HDFS架构" class="headerlink" title="1.3    HDFS架构"></a>1.3    HDFS架构</h4><h5 id="1-3-1-前提和设计目标"><a href="#1-3-1-前提和设计目标" class="headerlink" title="1.3.1    前提和设计目标"></a>1.3.1    前提和设计目标</h5><ol>
<li>硬件错误<br> a)    硬件错误是常态而不是异常。<br> b)    HDFS可能由成百上千的服务器所构成，单机故障概率的存在意味着总有一部分服务器不工作的。<br> c)    错误检测和快速自动恢复是HDFS最核心架构目标。 </li>
<li>流式数据访问<br> a)    运行在HDFS上的应用需要流式访问它们的数据集。<br> b)    HDFS的设计重点是批处理，而不是交互处理。是高吞吐量而不是低延迟。<br> c)    为了提高数据的吞吐量，在关键方面修改POSIX的语义。</li>
<li>大规模数据集<br> a)    HDFS上的一个典型文件大小一般都在G字节至T字节。TB PB ZB<br> b)    HDFS支持大文件存储。<br> c)    单一HDFS实例能支撑数以千万计的文件。 </li>
<li>简单的一致性模型<br> a)    HDFS应用遵循“一次写入多次读取”的文件访问模型。<br> b)    简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。<br> c)    Map/Reduce应用或者网络爬虫应用都非常适合这个模型。</li>
<li>移动计算比移动数据更划算<br> a)    降低网络阻塞的影响，提高系统数据的吞吐量。<br> b)    将计算程序发送到数据所在的主机，比GB级别TB级别的数据移动更便捷。</li>
<li>异构软硬件平台间的可移植性<br> a)    HDFS在设计的时候就考虑到平台的可移植性。<br> b)    这种特性方便了HDFS作为大规模数据应用平台的推广</li>
</ol>
<h5 id="1-3-2-HDFS架构"><a href="#1-3-2-HDFS架构" class="headerlink" title="1.3.2    HDFS架构"></a>1.3.2    HDFS架构</h5><p>问题：<br>    100台服务器，存储空间单个100GB 10T<br>    5T文件如何存储？</p>
<p>128MB一块      128MB<em>8=1GB    128</em>8*1024=1TB<br>5T数据分成的128MB的块数8192 *5。</p>
<p>清单：<br>    5TB文件分的块：<br>    元数据：</p>
<pre class="line-numbers language-none"><code class="language-none">文件名称：web.log
		大小：5TB
		创建时间：
		权限：
		文件所有者：
		文件所属的用户组：
		文件类型：
		
		文件块列表信息：
		0~128*1024*1024 -1：128MB：node1：path,node3:path,node8:path
		128*1024*1024~2*128*1024*1024 -1：128MB：node2：path,...
		2*128*1024*1024~3*128*1024*1024 -1：128MB：node3：path
		0~128*1024*1024 -1：128MB：node1：
		0~128*1024*1024 -1：128MB：node1：
		0~128*1024*1024 -1：128MB：node1：
		0~128*1024*1024 -1：128MB：node1：
		0~128*1024*1024 -1：128MB：node1：<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><a target="_blank" rel="noopener" href="https://ibb.co/r7Px01F">清单</a><br><a target="_blank" rel="noopener" href="https://ibb.co/87YSdjF">图例</a></p>
<h5 id="1-3-3-NameNode"><a href="#1-3-3-NameNode" class="headerlink" title="1.3.3    NameNode"></a>1.3.3    NameNode</h5><p>NameNode管理文件系统的命名空间</p>
<ol>
<li><p>文件和目录的元数据：(运行时，元数据放内存)<br> 文件的block副本个数–通常是3个<br> 修改和访问的时间<br> 访问权限<br> block大小以及组成文件的block列表信息</p>
</li>
<li><p>以两种方式在NameNode本地进行持久化：<br> 命名空间镜像文件（fsimage）和编辑日志（edits log）</p>
</li>
<li><p>   fsimage文件不记录每个block所在的DataNode信息，这些信息在每次系统启动的时候从DataNode重建。之后DataNode会周期性地通过心跳包向NameNode报告block信息。DataNode向NameNode注册的时候NameNode请求DataNode发送block列表信息。</p>
<pre class="line-numbers language-none"><code class="language-none">1、文件名称和路径
2、文件的大小
3、文件的所属关系
4、文件的block块大小  128MB  
5、文件的副本个数  3   MR  10个副本
6、文件的修改时间
7、文件的访问时间
8、文件的权限
9、文件的block列表
	blk1:0,134217728‬,node1,node13,node26：blockID
	blk2:134217728,134217728‬,node7,node89,node1002
	blk2:134217728*2,134217728‬,node7,node89,node1002
	blk2:134217728*3,134217728‬,node7,node89,node1002
	blk2:134217728*4,134217728‬,node7,node89,node1002
	blk2:134217728*5,134217728‬,node7,node89,node1002
	blk2:134217728,134217728‬,node7,node89,node1002
	blk2:134217728,134217728‬,node7,node89,node1002
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ol>
<p>存储结构<br>一个运行的NameNode如下的目录结构，该目录结构在第一次格式化的时候创建<br><a target="_blank" rel="noopener" href="https://ibb.co/0M0h5mV">存储结构</a><br>如果属性dfs.namenode.name.dir指定了多个路径，则每个路径中的内容是一样的，尤其是当其中一个是挂载的NFS的时候，这种机制为管理提供了一些弹性。备份数据<br>in_use.lock文件用于NameNode锁定存储目录。这样就防止其他同时运行的NameNode实例使用相同的存储目录。<br>edits表示edits log日志文件<br>fsimage表示文件系统元数据镜像文件<br>NameNode在checkpoint之前首先要切换新的edits log文件，在切换时更新seen_txid的值。上次合并fsimage和editslog之后的第一个操作编号</p>
<p>VERSION文件是一个Java的属性文件<br>    layoutVersion是一个负数，定义了HDFS持久化数据结构的版本。这个版本数字跟hadoop发行的版本无关。当layout改变的时候，该数字减1（比如从-57到-58）。当对HDFDS进行了升级，就会发生layoutVersion的改变。<br>    namespaceID是该文件系统的唯一标志符，当NameNode第一次格式化的时候生成。<br>    clusterID是HDFS集群使用的一个唯一标志符，在HDFS联邦的情况下，就看出它的作用了，因为联邦情况下，集群有多个命名空间，不同的命名空间由不同的NameNode管理。<br>    blockpoolID是block池的唯一标志符，一个NameNode管理一个命名空间，该命名空间中的所有文件存储的block都在block池中。<br>    cTime标记着当前NameNode创建的时间。对于刚格式化的存储，该值永远是0，但是当文件系统更新的时候，这个值就会更新为一个时间戳。<br>    storageType表示当前目录存储NameNode内容的数据结构</p>
<p>当文件系统客户端进行了写操作（例如创建或移动了文件），这个事务首先在edits log中记录下来。NameNode在内存中有文件系统的元数据，当edits log记录结束后，就更新内存中的元数据。内存中的元数据用于响应客户端的读请求。</p>
<p>edits log在磁盘上表现为一定数量的文件。每个文件称为片段（Segment），前缀“edits”，后缀是其中包含的事务ID（transaction IDs）。每个写操作事务都仅仅打开一个文件（比如：edits_inprogress_00000000000010），写完后冲刷缓冲区并同步到磁盘，然后返回客户端success状态码。如果NameNode的元数据需要写到多个目录中，则对于每个写事务需要所有的写操作都完成，并冲刷缓冲区同步到磁盘才返回success状态码。这样就可以保证在发生宕机的时候没有事务数据丢失。</p>
<p>用户的操作是一个事务，每个操作NN都要先将操作记录到edits log中，如果给NN指定了多个目录，则在多个目录中都存在edits log文件，用户的操作要在多个目录中都写完成，才让NN同步数据到内存中。当NN在内存中也同步了数据，就返回客户端success。</p>
<p>每个fsimage文件都是系统元数据的一个完整的持久化检查点（checkpoint）（后缀表示镜像中的最后一个事务）。写操作不更新这个数据，因为镜像文件通常为GB数量级，写到磁盘很慢。如果NameNode宕机，可以将最新fsimage加载到内存，同时执行edits log对应于该fsimage之后的操作，就可以重建元数据的状态。而这正是每次启动NameNode的时候NameNode要做的工作。</p>
<h5 id="1-3-4-SecondaryNameNode"><a href="#1-3-4-SecondaryNameNode" class="headerlink" title="1.3.4    SecondaryNameNode"></a>1.3.4    SecondaryNameNode</h5><p>存在的意义<br>edits log会随着对文件系统的操作而无限制地增长，这对正在运行的NameNode而言没有任何影响，如果NameNode重启，则需要很长的时间执行edits log的记录以更新fsimage（元数据镜像文件）。在此期间，整个系统不可用。<br>在系统启动期间，NameNode合并fsimage+edits log</p>
<p>fsimage=0<br>edist log=很大</p>
<p>内存<br>fsimage=GB<br>edits log<br>内存-&gt;执行edits log条目</p>
<p>解决方案就是运行SecondaryNameNode，它的作用就是为NameNode内存中的文件系统元数据生成检查点（checkpoint）。fsimage</p>
<p><strong>工作流程</strong><br>edits_inprogress_00000000018_0000000028  seen_txid=29<br>1、secondarynamenode请求namenode生成新的edits log文件并向其中写日志。NameNode会在所有的存储目录中更新seen_txid文件<br>2、SecondaryNameNode通过HTTP GET的方式从NameNode下载fsimage和edits文件到本地。<br>3、SecondaryNameNode将fsimage加载到自己的内存，并根据edits log更新内存中的fsimage信息，然后将更新完毕之后的fsimage写到磁盘上。<br>4、SecondaryNameNode通过HTTP PUT将新的fsimage文件发送到NameNode，NameNode将该文件保存为.ckpt的临时文件备用。<br>5、NameNode重命名该临时文件并准备使用。此时NameNode拥有一个新的fsimage文件和一个新的很小的edits log文件（可能不是空的，因为在SecondaryNameNode合并期间可能对元数据进行了读写操作）。管理员也可以将NameNode置于safemode，通过hdfs dfsadmin -saveNamespace命令来进行edits log和fsimage的合并</p>
<p>SecondaryNameNode要和NameNode拥有相同的内存。对大的集群，SecondaryNameNode运行于一台专用的物理主机。<br><a target="_blank" rel="noopener" href="https://ibb.co/m4W0VBn">图例</a></p>
<p>检查点创建时机<br>对于创建检查点（checkpoint）的过程，有三个参数进行配置：</p>
<p>1、默认情况下，SecondaryNameNode每个小时进行一次checkpoint合并<br>    由dfs.namenode.checkpoint.period设置，单位秒</p>
<p>2、在不足一小时的情况下，如果edits log存储的事务达到了1000000个也进行一次checkpoint合并<br>    由dfs.namenode.checkpoint.txns设置事务数量</p>
<p>3、事务数量检查默认每分钟进行一次<br>    由dfs.namenode.checkpoint.check.period设置，单位秒。</p>
<p>总结：<br>namenode<br>管理文件元数据<br>    文件名称、大小、所属关系、权限、副本大小、副本个数<br>    文件块的列表信息：(块的ID，偏移量，块的大小，块所在的主机名称列表)<br>持久化文件<br>    fsimage(内存快照)，edits log<br>    fsimage很大，GB级别；edits log只追加的文件<br>    用户操作首先记录到edits log中，然后更新内存<br>fsimage不保存数据块位置信息<br>    在系统启动的时候，datanode向namenode发送文件块列表信息（bid）<br>    datanode通过心跳向namenode汇报列表信息。<br>namenode元数据正常工作时，元数据放内存，高并发。<br>secondarynamenode<br>在系统启动的时候，namenode首先加载fsimage，然后逐条执行edits log中的日志操作，如果edits log很大，则需要很长时间才能加载完毕，向磁盘写新的fsimage，之后才可以对外提供服务。<br>周期性地从namenode拷贝fsimage+edits log，在SNN中合并为新的fsimage，推送给namenode。<br>条件：1、每小时一次，2、不足一小时，则只要edits log中记录的事务数达到了1000000，则合并。<br>datanode<br>     储存数据节点</p>
<p> <strong>存储结构</strong><br><a target="_blank" rel="noopener" href="https://ibb.co/0M0h5mV">存储结构</a><br>1、SecondaryNameNode中checkpoint目录布局（dfs.namenode.checkpoint.dir）和NameNode中的一样。</p>
<p>2、如果NameNode完全坏掉（没有备用机，也没有NFS），可以快速地从SecondaryNameNode恢复。有可能丢数据</p>
<p>3、如果SecondaryNameNode直接接手NameNode的工作，需要在启动NameNode进程的时候添加-importCheckpoint选项。该选项会让NameNode从由dfs.namenode.checkpoint.dir属性定义的路径中加载最新的checkpoint数据，但是为了防止元数据的覆盖，要求dfs.namenode.name.dir定义的目录中没有内容。</p>
<h5 id="1-3-5-DataNode"><a href="#1-3-5-DataNode" class="headerlink" title="1.3.5    DataNode"></a>1.3.5    DataNode</h5><p>存储结构<br>DataNode不需要显式地格式化;关键文件和目录结构如下<br><a target="_blank" rel="noopener" href="https://ibb.co/Swq7Q41">结构</a><br>1、HDFS块数据存储于blk_前缀的文件中，包含了被存储文件原始字节数据的一部分。</p>
<p>2、每个block文件都有一个.meta后缀的元数据文件关联。该文件包含了一个版本和类型信息的头部，后接该block中每个部分的校验和。</p>
<p>3、每个block属于一个block池，每个block池有自己的存储目录，该目录名称就是该池子的ID（跟NameNode的VERSION文件中记录的block池ID一样）。</p>
<p>当一个目录中的block达到64个（通过dfs.datanode.numblocks配置）的时候，DataNode会创建一个新的子目录来存放新的block和它们的元数据。这样即使当系统中有大量的block的时候，目录树也不会太深。同时也保证了在每个目录中文件的数量是可管理的，避免了多数操作系统都会碰到的单个目录中的文件个数限制（几十几百上千个）。</p>
<p>如果dfs.datanode.data.dir指定了位于在不同的硬盘驱动器上的多个不同的目录，则会通过轮询的方式向目录中写block数据。需要注意的是block的副本不会在同一个DataNode上复制，而是在不同的DataNode节点之间复制。</p>
<p><strong>存储数据模型(重点)</strong><br>1、文件线性切割成块（Block）（按字节切割）<br>…..<br>Hello world<br>2、Block分散存储在集群节点中<br>3、单一文件Block大小一致，文件与文件可以不一致<br>    hdfs  dfs  -D  dfs.blocksize=1048576  -D dfs.replication=2  -put hello.txt  /<br>4、Block可以设置副本数，副本分散在不同节点中<br>    a) 副本数不要超过节点数量<br>    b) 承担计算<br>    c) 容错<br>5、文件上传可以设置Block大小和副本数<br>6、已上传的文件Block副本数可以调整，大小不变<br>7、只支持一次写入多次读取，同一时刻只有一个写入者<br>    对同一个文件，一个时刻只有一个写入者<br>8、可以append追加数据</p>
<p>优势（了解）</p>
<ol>
<li>   一个文件的大小可以大于网络中任意一个磁盘的容量</li>
<li>   使用抽象块而非整个文件作为存储单元，大大简化存储子系统的设计</li>
<li>   块非常适合用于数据备份进而提供数据容错能力和提高可用性</li>
</ol>
<h4 id="1-4-数据块副本放置策略"><a href="#1-4-数据块副本放置策略" class="headerlink" title="1.4    数据块副本放置策略"></a>1.4    数据块副本放置策略</h4><p><a target="_blank" rel="noopener" href="https://ibb.co/YWFW01M">图例</a><br>Block的副本放置策略</p>
<p>第一个副本：放置在上传文件的DN；如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。<br>第二个副本：放置在于第一个副本不同的 机架的节点上。<br>第三个副本：与第二个副本相同机架的节点。<br>更多副本：随机节点</p>
<h4 id="1-5-HDFS的权限（了解）"><a href="#1-5-HDFS的权限（了解）" class="headerlink" title="1.5    HDFS的权限（了解）"></a>1.5    HDFS的权限（了解）</h4><p>1、每个文件和目录都和一个拥有者和组相关联。<br>2、文件或者目录对与拥有者、同组用户和其他用户拥有独立的权限。<br>3、对于一个文件，r表示读取的权限，w表示写或者追加的权限。对于目录而言，r表示列出目录内容的权限，w表示创建或者删除文件和目录的权限，x表示访问该目录子项目的权限。<br>4、默认情况下hadoop运行时安全措施处于停用模式。一个客户端可以在远程系统上通过创建和任意一个合法用户同名的账号来进行访问。 hadoop  root<br>5、安全措施可以防止用户或自动工具及程序意外修改或删除文件系统的重要部分。（dfs.permissions.enabled属性）。防止好人做错事。<br>6、超级用户是namenode进程的标识。对于超级用户，系统不会执行任何权限检查</p>
<h4 id="1-6-hadoop的安全模式"><a href="#1-6-hadoop的安全模式" class="headerlink" title="1.6    hadoop的安全模式"></a>1.6    hadoop的安全模式</h4><h5 id="1-6-1-工作流程-理解"><a href="#1-6-1-工作流程-理解" class="headerlink" title="1.6.1    工作流程(理解)"></a>1.6.1    工作流程(理解)</h5><ol>
<li>   启动NameNode，NameNode加载fsimage到内存，对内存数据执行edits log日志中的事务操作。</li>
<li>   文件系统元数据内存镜像加载完毕，进行fsimage和edits log日志的合并，并创建新的fsimage文件和一个空的edits log日志文件。</li>
<li>   NameNode等待DataNode上传block列表信息，直到副本数满足最小副本条件。</li>
<li>   当满足了最小副本条件，再过30秒，NameNode就会退出安全模式。最小副本条件指整个文件系统中有99.9%的block达到了最小副本数（默认值是1，可设置）</li>
</ol>
<p>在NameNode安全模式（safemode）</p>
<ol>
<li>   对文件系统元数据进行只读操作</li>
<li>   当文件的所有block信息具备的情况下，对文件进行只读操作</li>
<li>   不允许进行文件修改（写，删除或重命名文件）</li>
</ol>
<h5 id="1-6-2-注意事项"><a href="#1-6-2-注意事项" class="headerlink" title="1.6.2    注意事项"></a>1.6.2    注意事项</h5><ol>
<li>   NameNode不会持久化block位置信息；DataNode保有各自存储的block列表信息。正常操作时，NameNode在内存中有一个block位置的映射信息。</li>
<li>   NameNode在安全模式，NameNode需要给DataNode时间来上传block列表信息到NameNode。如果NameNode不等待DataNode上传这些信息的话，则会在DataNode之间进行block的复制，而这在大多数情况下都是非必须的（因为只需要等待DataNode上传就行了），还会造成资源浪费。</li>
<li>   在安全模式NameNode不会要求DataNode复制或删除block。</li>
<li>   新格式化的HDFS不进入安全模式，因为DataNode压根就没有block。</li>
</ol>
<h4 id="1-6-4-命令操作-了解"><a href="#1-6-4-命令操作-了解" class="headerlink" title="1.6.4    命令操作(了解)"></a>1.6.4    命令操作(了解)</h4><p>通过命令查看namenode是否处于安全模式：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">$ hdfs dfsadmin -safemode get
Safe mode is ON<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>HDFS的前端webUI页面也可以查看NameNode是否处于安全模式。<br>有时候我们希望等待安全模式退出，之后进行文件的读写操作，尤其是在脚本中，此时：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">$ hdfs dfsadmin -safemode wait
# your read or write command goes here<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>管理员有权在任何时间让namenode进入或退出安全模式。进入安全模式：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">$ hdfs dfsadmin -safemode enter
Safe mode is ON<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>这样做可以让namenode一直处于安全模式，也可以设置dfs.namenode.safemode.threshold-pct为1做到这一点。<br>离开安全模式：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">$ hdfs dfsadmin -safemode leave
Safe mode is OFF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="1-7-HDFS写文件流程（重点）"><a href="#1-7-HDFS写文件流程（重点）" class="headerlink" title="1.7    HDFS写文件流程（重点）"></a>1.7    HDFS写文件流程（重点）</h4><h5 id="1-7-1-流程"><a href="#1-7-1-流程" class="headerlink" title="1.7.1    流程"></a>1.7.1    流程</h5><p><a target="_blank" rel="noopener" href="https://ibb.co/K25Y9Pm">流程</a></p>
<ol>
<li>   调用客户端的对象DistributedFileSystem的create方法；</li>
<li>   DistributedFileSystem会发起对namenode的一个RPC连接，请求创建一个文件，不包含关于block块的请求。namenode会执行各种各样的检查，确保要创建的文件不存在，并且客户端有创建文件的权限。如果检查通过，namenode会创建一个文件（在edits中，同时更新内存状态），否则创建失败，客户端抛异常IOException。</li>
<li>   DistributedFileSystem返回一个FSDataOutputStream对象给客户端用于写数据。FSDataOutputStream封装了一个DFSOutputStream对象负责客户端跟datanode以及namenode的通信。</li>
<li>   FSDataOutputStream对象将数据切分为小的数据包（64kb，core-default.xml：file.client-write-packet-size默认值65536），并写入到一个内部队列（“数据队列”）。DataStreamer会读取其中内容，并请求namenode返回一个datanode列表来存储当前block副本。列表中的datanode会形成管线，DataStreamer将数据包发送给管线中的第一个datanode，第一个datanode将接收到的数据发送给第二个datanode，第二个发送给第三个。。。</li>
<li>   DFSOoutputStream维护着一个数据包的队列，这的数据包是需要写入到datanode中的，该队列称为确认队列。当一个数据包在管线中所有datanode中写入完成，就从ack队列中移除该数据包。如果在数据写入期间datanode发生故障，则执行以下操作<br>a)    关闭管线，把确认队列中的所有包都添加回数据队列的最前端，以保证故障节点下游的datanode不会漏掉任何一个数据包。<br>b)    为存储在另一正常datanode的当前数据块指定一个新的标志，并将该标志传送给namenode，以便故障datanode在恢复后可以删除存储的部分数据块。<br>c)    从管线中删除故障数据节点并且把余下的数据块写入管线中另外两个正常的datanode。namenode在检测到副本数量不足时，会在另一个节点上创建新的副本。<br>d)    后续的数据块继续正常接受处理。<br>e)    在一个块被写入期间可能会有多个datanode同时发生故障，但非常少见。只要设置了dfs.replication.min的副本数（默认为1），写操作就会成功，并且这个块可以在集群中异步复制，直到达到其目标副本数（dfs.replication默认值为3）。</li>
<li>   如果有多个block，则会反复从步骤4开始执行。</li>
<li>   当客户端完成了数据的传输，调用数据流的close方法。该方法将数据队列中的剩余数据包写到datanode的管线并等待管线的确认</li>
<li>   客户端收到管线中所有正常datanode的确认消息后，通知namenode文件写完了。</li>
<li>   namenode已经知道文件由哪些块组成，所以它在返回成功前只需要等待数据块进行最小量的复制。</li>
</ol>
<h4 id="1-8-HDFS读文件流程（重点）"><a href="#1-8-HDFS读文件流程（重点）" class="headerlink" title="1.8    HDFS读文件流程（重点）"></a>1.8    HDFS读文件流程（重点）</h4><h5 id="1-8-1-流程"><a href="#1-8-1-流程" class="headerlink" title="1.8.1    流程"></a>1.8.1    流程</h5><p><a target="_blank" rel="noopener" href="https://ibb.co/t393fxJ">流程</a></p>
<ol>
<li>   客户端通过FileSystem对象的open方法打开希望读取的文件，DistributedFileSystem对象通过RPC调用namenode，以确保文件起始位置。对于每个block，namenode返回存有该副本的datanode地址。这些datanode根据它们与客户端的距离来排序。如果客户端本身就是一个datanode，并保存有相应block一个副本，会从本地读取这个block数据。</li>
<li>   DistributedFileSystem返回一个FSDataInputStream对象给客户端读取数据。该类封装了DFSInputStream对象，该对象管理着datanode和namenode的I/O，用于给客户端使用。客户端对这个输入调用read方法，存储着文件起始几个block的datanode地址的DFSInputStream连接距离最近的datanode。通过对数据流反复调用read方法，可以将数据从datnaode传输到客户端。到达block的末端时，DFSInputSream关闭与该datanode的连接，然后寻找下一个block的最佳datanode。客户端只需要读取连续的流，并且对于客户端都是透明的。</li>
<li>   客户端从流中读取数据时，block是按照打开DFSInputStream与datanode新建连接的顺序读取的。它也会根据需要询问namenode来检索下一批数据块的datanode的位置。一旦客户端完成读取，就close掉FSDataInputStream的输入流。</li>
<li>   在读取数据的时候如果DFSInputStream在与datanode通信时遇到错误，会尝试从这个块的一个最近邻datanode读取数据。它也记住那个故障datanode，保证以后不会反复读取该节点上后续的block。DFSInputStream也会通过校验和确认从datanode发来的数据是否完整。如果发现有损坏的块，就在DFSInputStream试图从其他datanode读取其副本之前通知namenode。</li>
<li>   Client下载完block后会验证DN中的MD5，保证块数据的完整性。</li>
</ol>
<h5 id="1-8-2-注意"><a href="#1-8-2-注意" class="headerlink" title="1.8.2    注意"></a>1.8.2    注意</h5><p>namenode告知客户端每个block中最佳的datanode，并让客户端<strong>直接</strong>连到datanode检索数据。由于数据流分散在集群中的所有datanode，这样可以使HDFS可扩展到大量的并发客户端。同时，namenode只需要响应block位置的请求，无需响应数据请求，否则namenode会成为瓶颈。</p>
<h5 id="1-8-3-最近邻（了解）"><a href="#1-8-3-最近邻（了解）" class="headerlink" title="1.8.3    最近邻（了解）"></a>1.8.3    最近邻（了解）</h5><p>hadoop把网络看作是一棵树，两个节点间的距离是它们到最近共同祖先的距离和。通常可以设置等级：</p>
<ol>
<li>   同一个节点上的进程</li>
<li>   同一机架上的不同节点</li>
<li>   同一数据中心中不同机架上的节点</li>
<li>   不同数据中心中的节点</li>
</ol>
<h4 id="1-9-伪分布式搭建"><a href="#1-9-伪分布式搭建" class="headerlink" title="1.9    伪分布式搭建"></a>1.9    伪分布式搭建</h4><p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/">hadoop官网地址</a><br><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r1.0.4/cn">中文文档</a><br>安装&amp;部署:<br>node1:        NN        DN        SNN<br>1)基础设施<br>        GUN/Linux<br>        jdk  1.7+<br>        环境:JAVA_HOME -&gt;  /etc/profile   ~/.bash_profile<br>        ssh免密:<br>            远程执行&lt;-不需要用户交互,而是用户直接给出一个命令,直接在远程执行<br>                    不会加载 /etc/profile<br>            远程登陆&lt;-返回一个交互接口<br>                    返回接口/bash  会加载/etc/profile<br>            ip<br>            时间同步<br>            hosts/hostname<br>2)应用的安装<br>            a)绿色:开箱即用(配置/部署)<br>                环境变量<br>                应用自己的环境变量<br>                JAVA_HOME -&gt;  hadoop-env.sh<br>            b)操作:<br>                format<br>                start<br>                使用</p>
<h5 id="1-9-1-上传hadoop的tar包和jdk的rpm包"><a href="#1-9-1-上传hadoop的tar包和jdk的rpm包" class="headerlink" title="1.9.1    上传hadoop的tar包和jdk的rpm包"></a>1.9.1    上传hadoop的tar包和jdk的rpm包</h5><p>hadoop-2.6.5.tar.gz<br>jdkxxx.rpm<br>将文件上传到/opt/apps目录下</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 apps]# tar -zxvf hadoop-2.6.5.tar.gz -C &#x2F;opt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<h5 id="1-9-2-安装jdk并配置环境变量"><a href="#1-9-2-安装jdk并配置环境变量" class="headerlink" title="1.9.2    安装jdk并配置环境变量"></a>1.9.2    安装jdk并配置环境变量</h5><pre class="line-numbers language-linux" data-language="linux"><code class="language-linux">[root@node1 apps]# rpm -ivh jdk-8u221-linux-x64.rpm
[root@node1 apps]# vim  &#x2F;etc&#x2F;profile
export  JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;default
export  PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin
[root@node1 apps]# source  &#x2F;etc&#x2F;profile  或者  .  &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="1-9-3-配置免密钥"><a href="#1-9-3-配置免密钥" class="headerlink" title="1.9.3    配置免密钥"></a>1.9.3    配置免密钥</h5><pre class="line-numbers language-linux" data-language="linux"><code class="language-linux">tar  -zxf  hadoop-2.6.5.tar.gz  -C  &#x2F;opt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h5 id="1-9-4-解压hadoop-2-6-5-tar-gz到-opt目录"><a href="#1-9-4-解压hadoop-2-6-5-tar-gz到-opt目录" class="headerlink" title="1.9.4    解压hadoop-2.6.5.tar.gz到/opt目录"></a>1.9.4    解压hadoop-2.6.5.tar.gz到/opt目录</h5><pre class="line-numbers language-linux" data-language="linux"><code class="language-linux">tar  -zxf  hadoop-2.6.5.tar.gz  -C  &#x2F;opt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<h5 id="1-9-5-添加环境变量"><a href="#1-9-5-添加环境变量" class="headerlink" title="1.9.5    添加环境变量"></a>1.9.5    添加环境变量</h5><p>将HADOOP_HOME以及HADOOP_HOME/bin和HADOOP_HOME/sbin添加到环境变量</p>
<pre class="line-numbers language-sheel" data-language="sheel"><code class="language-sheel">
export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.6.5
export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h5 id="1-9-6-hadoop-env-sh配置"><a href="#1-9-6-hadoop-env-sh配置" class="headerlink" title="1.9.6    hadoop-env.sh配置"></a>1.9.6    hadoop-env.sh配置</h5><p>$HADOOP_HOME/etc/hadoop<br>由于通过SSH远程启动进程的时候默认不会加载/etc/profile设置，JAVA_HOME变量就加载不到，需要手动指定。</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_221-amd64
#或
export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;default
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h5 id="1-9-7-core-site-xml"><a href="#1-9-7-core-site-xml" class="headerlink" title="1.9.7    core-site.xml"></a>1.9.7    core-site.xml</h5><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定访问HDFS的时候路径的默认前缀  /  hdfs://node1:9000/ --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://node1:9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定hadoop的临时目录位置，它会给namenode、secondarynamenode以及datanode的存储目录指定前缀 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/var/bjsxt/hadoop/pseudo<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>配置文件拷贝后格式不美观，可以通过以下方式格式化：</p>
<ol>
<li>   拷贝到对应的文件</li>
<li>Esc-&gt;Ctrl+V (下箭头选择要格式化的代码）<br> :!xmllint -format -<br> dd:再删除多没有&lt;?xml ….&gt;</li>
<li>   拷贝格式化后的代码，然后回车</li>
<li>   dG删除非格式的代码</li>
<li>   i-&gt;Shift+Ins将格式后的内容拷贝到文件中</li>
<li>   保存并退出</li>
</ol>
<h5 id="1-9-8-hdfs-site-xml"><a href="#1-9-8-hdfs-site-xml" class="headerlink" title="1.9.8    hdfs-site.xml"></a>1.9.8    hdfs-site.xml</h5><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定block副本数 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1：：<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定secondarynamenode所在的位置 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node1:50090<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="1-9-9-slaves"><a href="#1-9-9-slaves" class="headerlink" title="1.9.9    slaves"></a>1.9.9    slaves</h5><p>DataNode所在的节点</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@bk1 hadoop]# vim slaves
node1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h5 id="1-9-10-格式化"><a href="#1-9-10-格式化" class="headerlink" title="1.9.10    格式化"></a>1.9.10    格式化</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">hdfs  namenode  -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h5 id="1-9-11-启动"><a href="#1-9-11-启动" class="headerlink" title="1.9.11    启动"></a>1.9.11    启动</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">start-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h5 id="1-9-12-查看进程"><a href="#1-9-12-查看进程" class="headerlink" title="1.9.12    查看进程"></a>1.9.12    查看进程</h5><pre class="line-numbers language-linux" data-language="linux"><code class="language-linux">[root@node1 current]# jps
1943 SecondaryNameNode
1800 DataNode
1693 NameNode
2045 Jps<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>说明进程都正常启动了，然后网页访问：<a target="_blank" rel="noopener" href="http://node1:50070/">http://node1:50070</a></p>
<h5 id="1-9-13-上传文件"><a href="#1-9-13-上传文件" class="headerlink" title="1.9.13    上传文件"></a>1.9.13    上传文件</h5><p>生成本地文件：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">for i in &#96;seq 100000&#96;; do echo &quot;hello bjsxt $i&quot; &gt;&gt; hh.txt; done
ll -h

hdfs dfs -D dfs.blocksize&#x3D;1048576 -D dfs.replication&#x3D;1 -put hh.txt &#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h5 id="1-9-14-关闭"><a href="#1-9-14-关闭" class="headerlink" title="1.9.14    关闭"></a>1.9.14    关闭</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 ~]# stop-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="2-HDFS搭建"><a href="#2-HDFS搭建" class="headerlink" title="2 HDFS搭建"></a>2 HDFS搭建</h3><p>####1.1 目标<br>HDFS完全分布式搭建(熟练)<br>Hadoop 3.x 新特性（了解）<br>Hadoop Federation（了解）<br>Hadoop HA（掌握）<br>Hadoop HA 集群搭建（熟练）<br>java客户端操作HDFS（熟练）</p>
<h4 id="1-2-HDFS完全分布式搭建"><a href="#1-2-HDFS完全分布式搭建" class="headerlink" title="1.2    HDFS完全分布式搭建"></a>1.2    HDFS完全分布式搭建</h4><h5 id="1-2-1-规划"><a href="#1-2-1-规划" class="headerlink" title="1.2.1    规划"></a>1.2.1    规划</h5><p>四台服务器：<br>| node | node2 | node3 | node4 |<br>| – | – | – | – |<br>| NameNode | SecondaryNameNode |  |  |<br>| | DataNode-1 | DataNode-2 | DataNode -3 |</p>
<ol>
<li>基础设置<br> a)    网络<br> b)    Ssh: 哪个节点将公钥分发,成为启动start-dfs.sh脚本的主机和这个主机上的进程没有关系<br> c)    Jdk</li>
<li>应用搭建<br> a)    部署&amp;配置<br> b)    执行:</li>
</ol>
<h5 id="1-2-2-搭建步骤"><a href="#1-2-2-搭建步骤" class="headerlink" title="1.2.2    搭建步骤"></a>1.2.2    搭建步骤</h5><h6 id="1-2-2-1-免密钥设置"><a href="#1-2-2-1-免密钥设置" class="headerlink" title="1.2.2.1    免密钥设置"></a>1.2.2.1    免密钥设置</h6><p>四台服务器之间互相均可以免密登录<br>a、    首先在四台服务器上都要执行：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">ssh-keygen  -t  dsa  -P  &#39;&#39;  -f  ~&#x2F;.ssh&#x2F;id_dsa<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>b、在node1上将node1 的公钥拷贝到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>  将该文件拷贝给node2：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp  ~&#x2F;.ssh&#x2F;authorized_keys   node2:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>c、在node2中将node2的公钥追加到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>   将该文件拷贝给node3：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp  ~&#x2F;.ssh&#x2F;authorized_keys   node3:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>d、在node3中将node3的公钥追加到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>将该文件拷贝给node4：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp  ~&#x2F;.ssh&#x2F;authorized_keys   node4:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>e、在node4中将node4的公钥追加到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>将该文件拷贝给node1、node2、node3：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp  ~&#x2F;.ssh&#x2F;authorized_keys   node1:&#x2F;root&#x2F;.ssh&#x2F;
scp  ~&#x2F;.ssh&#x2F;authorized_keys   node2:&#x2F;root&#x2F;.ssh&#x2F;
scp  ~&#x2F;.ssh&#x2F;authorized_keys   node3:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h6 id="1-2-2-2-JDK安装环境变量配置"><a href="#1-2-2-2-JDK安装环境变量配置" class="headerlink" title="1.2.2.2    JDK安装环境变量配置"></a>1.2.2.2    JDK安装环境变量配置</h6><p>首先将node1中的hadoop-2.6.5删除，或者通过快照还原到单机伪分布安装前的环境。<br>node1-node4</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">mkdir &#x2F;opt&#x2F;apps<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>将jdk-8u221-linux-x64.rpm上传到node1/opt/apps<br>将/opt/apps下的jdk.rpm scp到node2、node3、node4的对应目录中</p>
<pre class="line-numbers language-none"><code class="language-none">scp jdk-8u221-linux-x64.rpm node2:&#96;pwd&#96;
scp jdk-8u221-linux-x64.rpm node3:&#96;pwd&#96;
scp jdk-8u221-linux-x64.rpm node4:&#96;pwd&#96;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>在node1、node2、node3、node4上安装jdk并配置profile文件</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">rpm -ivh jdk-8u221-linux-x64.rpm<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>将hadoop安装文件上传到node1的/opt/apps目录下，并解压到/opt目录下</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">tar  -zxvf  hadoop-2.6.5.tar.gz  -C  &#x2F;opt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>node1上修改环境变量</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.6.5
export 
PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>Node2上修改环境变量：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.6.5
export
PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$ZOOKEEPER_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>让配置文件生效</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>将node2的/etc/profile拷贝到node3、node4上并执行. /etc/profile</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp &#x2F;etc&#x2F;profile node[34]:&#96;pwd&#96;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h6 id="1-2-2-3-Hadoop相关配置"><a href="#1-2-2-3-Hadoop相关配置" class="headerlink" title="1.2.2.3    Hadoop相关配置"></a>1.2.2.3    Hadoop相关配置</h6><p>先在node1上配置好，然后将之scp到node2-node4上</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd &#x2F;opt&#x2F;hadoop-2.6.5&#x2F;etc&#x2F;hadoop&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ol>
<li><p>   hadoop-env.sh配置<br>$HADOOP_HOME/etc/hadoop<br>由于通过SSH远程启动进程的时候默认不会加载/etc/profile设置，JAVA_HOME变量就加载不到，需要手动指定。</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export  JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_221-amd64<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>   修改slaves指定datanode的位置</p>
<pre class="line-numbers language-none"><code class="language-none">node2
node3
node4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
<li><p>   修改hdfs-site.xml<br>指定SNN的位置</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node2:50090<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>   修改core-site.xml</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!--用来指定hdfs的老大，namenode的地址--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://node1:9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定hadoop的临时目录位置--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/var/bjsxt/hadoop/full<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>   拷贝到node2-node4上<br>先将之打成压缩包</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 opt]# tar -zcvf hadoop-2.6.5.tar.gz hadoop-2.6.5&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
</ol>
<p>将/opt/hadoop-2.6.5.tar.gz scp到node2、node3、node4的对应目录中</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp  hadoop-2.6.5.tar.gz node2:&#x2F;opt
scp  hadoop-2.6.5.tar.gz node3:&#x2F;opt
scp  hadoop-2.6.5.tar.gz node4:&#x2F;opt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>node2、node3、node4分别解压</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 opt]# tar -zxvf hadoop-2.6.5.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h6 id="1-2-2-4-格式化并启动"><a href="#1-2-2-4-格式化并启动" class="headerlink" title="1.2.2.4    格式化并启动"></a>1.2.2.4    格式化并启动</h6><p>格式化<br>在node1上执行：</p>
<pre class="line-numbers language-none"><code class="language-none">hdfs  namenode  -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>启动即可(该命令在四台服务器上哪一台执行都可以)</p>
<pre class="line-numbers language-none"><code class="language-none">start-dfs.sh
http:&#x2F;&#x2F;192.168.20.201:50070
[root@node1 opt]#hdfs dfs -mkdir -p &#x2F;user&#x2F;root
[root@node1 opt]# hdfs dfs -put hadoop-2.6.5.tar.gz &#x2F;user&#x2F;root<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h6 id="1-2-2-5-停止"><a href="#1-2-2-5-停止" class="headerlink" title="1.2.2.5    停止"></a>1.2.2.5    停止</h6><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">stop-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<h4 id="1-3-Hadoop-Federation（了解）联邦"><a href="#1-3-Hadoop-Federation（了解）联邦" class="headerlink" title="1.3    Hadoop Federation（了解）联邦"></a>1.3    Hadoop Federation（了解）联邦</h4><h5 id="1-3-1-NameNode需要多少内存"><a href="#1-3-1-NameNode需要多少内存" class="headerlink" title="1.3.1    NameNode需要多少内存"></a>1.3.1    NameNode需要多少内存</h5><p>问题：NameNode需要多大的内存？<br>业界看法：1GB内存放1，000，000block元数据<br>200个节点的集群中每个节点有24TB存储空间，block大小为128MB，block复制因子为3，能存储大概12500,000个block（或更多）：200×24,000,000MB/(128MB×3)。此时，NameNode内存大概需要12.5GB。</p>
<h5 id="1-3-2-HDFS组成"><a href="#1-3-2-HDFS组成" class="headerlink" title="1.3.2    HDFS组成"></a>1.3.2    HDFS组成</h5><p>1、Namespace<br>    a) 包括目录，文件和block块。<br>    b) 支持所有跟文件系统命名空间相关的操作<br>    如：创建、删除、修改和列出文件及目录。<br>2、Block存储服务包含两部分：<br>    a) NameNode中的block块管理<br>        i. 通过心跳机制和注册机制提供了对DataNode集群的管理。<br>        ii. 处理block块报告，管理block块的位置。<br>        iii. 提供跟block块相关的操作，如：创建、修改、删除和查询block块的位置。<br>        iv. 管理block副本如何放置，当副本数少于指定值之后增加副本，当副本数多于指定值之后删除副本。<br>    b) 存储：<br>        在DataNode本地文件系统中存储block块，并提供读/写访问。<br><a target="_blank" rel="noopener" href="https://ibb.co/3kpShZW">图例</a></p>
<p>1、NameNode节点之间是相互独立的联邦的关系，即它们之间不需要协调服务。<br>2、DataNode向集群中所有的NameNode注册，发送心跳和block块列表报告，处理来自NameNode的指令。<br>3、用户可以使用ViewFs创建个性化的命名空间视图，ViewFs类似于在Unix/Linux系统中的客户端挂载表。</p>
<p><strong>VERSION</strong><br>namespaceID  用于标记namenode的ID<br>blockpoolID  用于标记block存储池子的ID<br>clusterID  集群的ID</p>
<p><strong>Hadoop-env.sh</strong><br>配置JAVA_HOME<br><strong>core-site.xml配置：</strong></p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>viewfs://ClusterX<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.viewfs.mounttable.ClusterX.link./data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://node1:8020/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.viewfs.mounttable.ClusterX.link./project<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://node1:8020/project<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.viewfs.mounttable.ClusterX.link./user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://node2:8020/user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.viewfs.mounttable.ClusterX.link./tmp<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://node2:8020/tmp<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.viewfs.mounttable.ClusterX.linkFallback<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://node2:8020/home<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/var/bjsxt/hadoop/federation<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>hdfs-site.xml</strong></p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.blocksize<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1048576<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.nameservices<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>ns1,ns2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.rpc-address.ns1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node1:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address.ns1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node1:50070<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address.ns1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node3:50090<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.rpc-address.ns2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node2:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address.ns2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node2:50070<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address.ns2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node4:50090<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>slaves<br>node2<br>node3<br>node4</p>
<p>格式化node1</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">$HADOOP_PREFIX_HOME&#x2F;bin&#x2F;hdfs namenode -format -clusterId myviewfs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>格式化node2</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">$HADOOP_PREFIX_HOME&#x2F;bin&#x2F;hdfs namenode -format -clusterId myviewfs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>在格式化node1和node2上的namenode时候，需要指定clusterId，并且两个格式化的时候这个clusterId要一致，两个namenode具有相同的clusterId，它们在一个集群中，它们是联邦的关系<br>启动</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">start-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>配置</p>
<pre class="line-numbers language-hadoop" data-language="hadoop"><code class="language-hadoop">hdfs dfs -mkdir hdfs:&#x2F;&#x2F;node1:8020&#x2F;data
hdfs dfs -mkdir hdfs:&#x2F;&#x2F;node1:8020&#x2F;project
hdfs dfs -mkdir hdfs:&#x2F;&#x2F;node2:8020&#x2F;user
hdfs dfs -mkdir hdfs:&#x2F;&#x2F;node2:8020&#x2F;tmp
hdfs dfs -mkdir hdfs:&#x2F;&#x2F;node2:8020&#x2F;home<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>停止</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">stop-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h5 id="1-3-3-优点"><a href="#1-3-3-优点" class="headerlink" title="1.3.3    优点"></a>1.3.3    优点</h5><p>1、    通过多个namenode/namespace把元数据的存储和管理分散到多个节点中，使得namenode/namespace可以通过增加机器来进行水平扩展。<br>2、    能把单个namenode的负载分散到多个节点中，在HDFS数据规模较大的时候不会也降低HDFS的性能。<br>3、    可以通过多个namespace来隔离不同类型的应用，把不同类型应用的HDFS元数据的存储和管理分派到不同的namenode中</p>
<h4 id="1-4-Hadoop-NameNode-HA"><a href="#1-4-Hadoop-NameNode-HA" class="headerlink" title="1.4    Hadoop NameNode HA"></a>1.4    Hadoop NameNode HA</h4><p>HDFS  2.x<br>解决HDFS 1.0中单点故障和内存受限问题，联邦     HA<br>HDFS2.x中Federation和HA分离，HA只能有两个NameNode<br><strong>解决单点故障</strong><br>HDFS HA：通过主备NameNode解决<br>如果主NameNode发生故障，则切换到备NameNode上。<br><strong>解决内存受限问题</strong><br>HDFS Federation(联邦)；水平扩展，支持多个NameNode；<br>（1）所有NameNode共享所有DataNode存储资源<br>（2）每个NameNode分管一部分目录</p>
<h5 id="1-4-1-手动HA"><a href="#1-4-1-手动HA" class="headerlink" title="1.4.1    手动HA"></a>1.4.1    手动HA</h5><p>fsimage+edits log需要<br><a target="_blank" rel="noopener" href="https://ibb.co/b2cg9Gh">图例</a><br>由StandbyNameNode做合并工作<br>fsimage推送的时机可以通过参数来调整：<br>dfs.namenode.checkpoint.period    1小时<br>dfs.namenode.checkpoint.txns      100 0000事务<br>dfs.namenode.checkpoint.check.period   3s<br>dfs.namenode.num.checkpoints.retained<br>dfs.ha.tail-edits.period</p>
<p>1、    一个NameNode进程处于Active状态，另1个NameNode进程处于Standby状态。Active的NameNode负责处理客户端的请求。<br>2、    Active的NN修改了元数据之后，会在JNs的半数以上的节点上记录这个日志。Standby状态的NameNode会监视任何对JNs上edit log的更改。一旦edits log出现更改，Standby的NN就会根据edits log更改自己记录的元数据。<br>3、    当发生故障转移时，Standby主机会确保已经读取了JNs上所有的更改来同步它本身记录的元数据，然后由Standby状态切换为Active状态。<br>4、    为了确保在发生故障转移操作时拥有相同的数据块位置信息，DNs向所有NN发送数据块位置信息和心跳数据。<br>5、    JNs只允许一台NameNode向JNs写edits log数据，这样就能保证不会发生“脑裂</p>
<h5 id="1-4-2-自动HA"><a href="#1-4-2-自动HA" class="headerlink" title="1.4.2    自动HA"></a>1.4.2    自动HA</h5><p><a target="_blank" rel="noopener" href="https://ibb.co/mJLWCJK">图例</a></p>
<h5 id="1-4-3-总结"><a href="#1-4-3-总结" class="headerlink" title="1.4.3    总结"></a>1.4.3    总结</h5><p>主备NameNode<br>解决单点故障（属性，位置）元数据<br>主NameNode对外提供服务，备NameNode同步主NameNode元数据，以待切换<br>所有DataNode同时向两个NameNode汇报数据块信息（位置）<br>JNN:集群（属性）同步edits log<br>standby：备，完成了edits.log文件的合并产生新的image，推送回ANN<br><strong>两种切换选择</strong><br>手动切换：通过命令实现主备之间的切换，可以用HDFS升级等场合<br>自动切换：基于Zookeeper实现<br><strong>基于Zookeeper自动切换方案</strong><br>    ZooKeeper Failover Controller：监控NameNode健康状态，并向Zookeeper注册NameNode。NameNode挂掉后，ZKFC为NameNode竞争锁，获得ZKFC 锁的NameNode变为active<br>zookeeper的分布式锁，keepalived</p>
<h3 id="1-5-Hadoop-HA-集群搭建"><a href="#1-5-Hadoop-HA-集群搭建" class="headerlink" title="1.5    Hadoop HA 集群搭建"></a>1.5    Hadoop HA 集群搭建</h3><h4 id="1-5-1-规划"><a href="#1-5-1-规划" class="headerlink" title="1.5.1    规划"></a>1.5.1    规划</h4><p>略》》 </p>
<h4 id="1-5-2-搭建步骤"><a href="#1-5-2-搭建步骤" class="headerlink" title="1.5.2    搭建步骤"></a>1.5.2    搭建步骤</h4><p><a target="_blank" rel="noopener" href="https://ibb.co/mttQpW2">图例</a><br>如何让ssh不提示fingerprint信息，然后输入yes或者no<br>/etc/ssh/ssh_config(客户端配置文件)  区别于sshd_config(服务端配置文件)</p>
<p> <a target="_blank" rel="noopener" href="https://ibb.co/wcW30sq">配置文件</a></p>
<h6 id="1-5-2-1-四台服务器之间免密登录"><a href="#1-5-2-1-四台服务器之间免密登录" class="headerlink" title="1.5.2.1    四台服务器之间免密登录"></a>1.5.2.1    四台服务器之间免密登录</h6><p> 四台服务器之间互相均可以免密登录<br>a、    首先在四台服务器上都要执行：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">ssh-keygen  -t  dsa  -P  &#39;&#39;  -f  ~&#x2F;.ssh&#x2F;id_dsa<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>b、在node1上将node1 的公钥拷贝到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>#将该文件拷贝给node2：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp  ~&#x2F;.ssh&#x2F;authorized_keys   node2:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>c、在node2中将node2的公钥追加到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys
#将该文件拷贝给node3：
scp  ~&#x2F;.ssh&#x2F;authorized_keys   node3:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>d、在node3中将node3的公钥追加到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys
#将该文件拷贝给node4：
scp  ~&#x2F;.ssh&#x2F;authorized_keys   node4:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>e、在node4中将node4的公钥追加到authorized_keys中：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat  ~&#x2F;.ssh&#x2F;id_dsa.pub  &gt;&gt;  ~&#x2F;.ssh&#x2F;authorized_keys
#将该文件拷贝给node1、node2、node3：
scp  ~&#x2F;.ssh&#x2F;authorized_keys   node1:&#x2F;root&#x2F;.ssh&#x2F;
scp  ~&#x2F;.ssh&#x2F;authorized_keys   node2:&#x2F;root&#x2F;.ssh&#x2F;
scp  ~&#x2F;.ssh&#x2F;authorized_keys   node3:&#x2F;root&#x2F;.ssh&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h6 id="1-5-2-2-JDK安装环境变量配置"><a href="#1-5-2-2-JDK安装环境变量配置" class="headerlink" title="1.5.2.2    JDK安装环境变量配置"></a>1.5.2.2    JDK安装环境变量配置</h6><p>首先将node1中的hadoop-2.6.5删除，或者通过快照还原到单机伪分布安装前的环境。<br>node1-node4</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;opt&#x2F;apps<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>将jdk-8u221-linux-x64.rpm上传到node1/opt/apps<br>将/opt/apps下的jdk.rpm scp到node2、node3、node4的对应目录中</p>
<pre class="line-numbers language-none"><code class="language-none">scp jdk-8u221-linux-x64.rpm node2:&#x2F;opt&#x2F;apps
scp jdk-8u221-linux-x64.rpm node3:&#x2F;opt&#x2F;apps
scp jdk-8u221-linux-x64.rpm node4:&#x2F;opt&#x2F;apps<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>在node1、node2、node3、node4上安装jdk并配置profile文件</p>
<pre class="line-numbers language-none"><code class="language-none">rpm -ivh jdk-8u221-linux-x64.rpm<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>node1上修改环境变量</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">vim &#x2F;etc&#x2F;profile
export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;default
export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>将node1的/etc/profile拷贝到node2、node3、node4上并执行. /etc/profile</p>
<pre class="line-numbers language-none"><code class="language-none">scp &#x2F;etc&#x2F;profile node[234]:&#96;pwd&#96;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>



<h6 id="1-5-2-3-zookeeper集群搭建"><a href="#1-5-2-3-zookeeper集群搭建" class="headerlink" title="1.5.2.3    zookeeper集群搭建"></a>1.5.2.3    zookeeper集群搭建</h6><p>a) 将zookeeper.tar.gz上传到node2<br>b) 解压到/opt</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">tar -zxvf zookeeper-3.4.6.tar.gz -C &#x2F;opt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>c) 配置环境变量：</p>
<pre class="line-numbers language-none"><code class="language-none">export ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;zookeeper-3.4.6
export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$ZOOKEEPER_HOME&#x2F;bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>然后./etc/profile让配置生效<br> 最后将该文件scp到node3和node4上，并分别./etc/profile让配置生效.</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp &#x2F;etc&#x2F;profile bk3:&#x2F;etc&#x2F;
scp &#x2F;etc&#x2F;profile bk4:&#x2F;etc&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>d) 到$ZOOKEEPER_PREFIX/conf下<br>复制zoo_sample.cfg为zoo.cfg</p>
<pre class="line-numbers language-none"><code class="language-none">cp zoo_sample.cfg  zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>e) 编辑zoo.cfg<br>添加如下行：<br>server.1=node2:2881:3881<br>server.2=node3:2881:3881<br>server.3=node4:2881:3881</p>
<p>修改<br>dataDir=/var/bjsxt/zookeeper/data</p>
<p>f) 创建/var/bjsxt/zookeeper/data目录，并在该目录下放一个文件：myid<br>在myid中写下当前zookeeper的编号</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">mkdir -p &#x2F;var&#x2F;bjsxt&#x2F;zookeeper&#x2F;data
echo 1 &gt; &#x2F;var&#x2F;bjsxt&#x2F;zookeeper&#x2F;data&#x2F;myid<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>g)将配置好zookeeper拷贝到node3、node4上</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">scp -r zookeeper-3.4.6&#x2F; bk3:&#x2F;opt&#x2F;
scp -r zookeeper-3.4.6&#x2F; bk4:&#x2F;opt&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>h) 在node3和node4上分别创建/var/bjsxt/zookeeper/data目录，<br>并在该目录下放一个文件：myid</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">node3
mkdir -p &#x2F;var&#x2F;bjsxt&#x2F;zookeeper&#x2F;data
echo 2 &gt; &#x2F;var&#x2F;bjsxt&#x2F;zookeeper&#x2F;data&#x2F;myid
node4
mkdir -p &#x2F;var&#x2F;bjsxt&#x2F;zookeeper&#x2F;data
echo 3 &gt; &#x2F;var&#x2F;bjsxt&#x2F;zookeeper&#x2F;data&#x2F;myid<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>i) 分别启动zookeeper<br>        zkServer.sh start 启动zk<br>        zkServer.sh stop  停止zk<br>        zkServer.sh status  查看zk状态<br>        zkServer.sh start|stop|status<br>j) 关闭zookeeper<br>        zkServer.sh stop<br>l) 连接zookeeper<br>        zkCli.sh     node2、node3、node4都可以<br>m) 退出zkCli.sh命令<br>        quit</p>
<h6 id="1-5-2-4-hadoop配置"><a href="#1-5-2-4-hadoop配置" class="headerlink" title="1.5.2.4    hadoop配置"></a>1.5.2.4    hadoop配置</h6><p><strong>一律在node1上操作，做完后scp到node2、node3、node4</strong></p>
<ol>
<li><p>   hadoop-env.sh配置JDK</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;default<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>core-site.xml</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://mycluster<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>   
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/var/bjsxt/hadoop/ha<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定每个zookeeper服务器的位置和客户端端口号 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
	 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>ha.zookeeper.quorum<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
	 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node2:2181,node3:2181,node4:2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>   hdfs-site.xml</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定副本的数量 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 解析参数dfs.nameservices值hdfs://mycluster的地址 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.nameservices<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mycluster<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- mycluster由以下两个namenode支撑 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.ha.namenodes.mycluster<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>nn1,nn2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定nn1地址和端口号  --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.rpc-address.mycluster.nn1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node1:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定nn2地址和端口号  --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.rpc-address.mycluster.nn2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node2:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定客户端查找active的namenode的策略：
       	       	  会给所有namenode发请求，以决定哪个是active的 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.client.failover.proxy.provider.mycluster<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 指定三台journal node服务器的地址 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.shared.edits.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>qjournal://node1:8485;node2:8485;node3:8485/mycluster<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.journalnode.edits.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/var/bjsxt/hadoop/ha/jnn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!-- 当active nn出现故障时，ssh到对应的服务器，将namenode进程kill掉  --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.ha.fencing.methods<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>sshfence<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.ha.fencing.ssh.private-key-files<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/root/.ssh/id_dsa<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment">&lt;!--启动NN故障自动切换 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.ha.automatic-failover.enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>   修改slaves指定datanode的位置</p>
<pre class="line-numbers language-none"><code class="language-none">[root@node1 hadoop]# vim slaves
node2
node3
node4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>   先同步配置文件到node2、node3、node4<br>node1上执行：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 opt]# tar -zcvf hadoop-2.6.5.tar.gz hadoop-2.6.5&#x2F;
[root@node1 opt]# scp hadoop-2.6.5.tar.gz node2:&#x2F;opt&#x2F;apps&#x2F;
[root@node1 opt]# scp hadoop-2.6.5.tar.gz node3:&#x2F;opt&#x2F;apps&#x2F;  
[root@node1 opt]# scp hadoop-2.6.5.tar.gz node4:&#x2F;opt&#x2F;apps&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ol>
<p>node2、node3、node4分别执行解压：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">tar -zxvf &#x2F;opt&#x2F;apps&#x2F;hadoop-2.6.5.tar.gz  -C &#x2F;opt&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>Hadoop环境变量配置：<br>node1上：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 opt]# vim &#x2F;etc&#x2F;profile
export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.6.5
export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>node2上：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 opt]# vim &#x2F;etc&#x2F;profile
export ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;zookeeper-3.4.6
export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.6.5
export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$ZOOKEEPER_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin
[root@node1 opt]#source &#x2F;etc&#x2F;profile  
[root@node1 opt]# scp &#x2F;etc&#x2F;profile bk3:&#x2F;etc&#x2F;   
[root@node1 opt]# scp &#x2F;etc&#x2F;profile bk4:&#x2F;etc&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>node3、node4分别执行:</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<h6 id="1-5-2-5-启动ha的hadoop"><a href="#1-5-2-5-启动ha的hadoop" class="headerlink" title="1.5.2.5    启动ha的hadoop"></a>1.5.2.5    启动ha的hadoop</h6><p>a)    启动zookeeper集群, node2、node3、node4分别执行:</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">zkServer.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>b)    在node1\node2\node3上启动三台journalnode</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">hadoop-daemon.sh start journalnode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>c)    选择node1，格式化HDFS</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>/var/bjsxt/hadoop/ha/dfs/name/current/目录下产生了fsimage文件<br>格式化后，启动namenode进程</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">hadoop-daemon.sh start namenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>d)    在另一台node2上同步元数据</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">hdfs namenode -bootstrapStandby<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>出现以下提示：</p>
<pre class="line-numbers language-none"><code class="language-none">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;
About to bootstrap Standby ID nn2 from:
           Nameservice ID: mycluster
        Other Namenode ID: nn1
  Other NN&#39;s HTTP address: http:&#x2F;&#x2F;node1:50070
  Other NN&#39;s IPC  address: node1&#x2F;192.168.20.201:8020
             Namespace ID: 178118551
            Block pool ID: BP-1909026874-192.168.20.201-1577760263511
               Cluster ID: CID-8105daf4-bdbb-40e8-a9d0-8d3f3867535b
           Layout version: -60
       isUpgradeFinalized: true
&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>e)    初始化zookeeper上的内容 一定是在namenode节点（node1或node2）上。<br>执行格式命令之前在node2-node4任一节点上：</p>
<pre class="line-numbers language-none"><code class="language-none">[root@node4 hadoop]# zkCli.sh
[zk: localhost:2181(CONNECTED) 0] ls &#x2F;
[zookeeper]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>只有默认的一个节点。<br>接下来在node1上执行：</p>
<pre class="line-numbers language-none"><code class="language-none">[root@node1 ~]# hdfs zkfc -formatZK<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p>然后在node4上接着执行</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;
[zookeeper, hadoop-ha]
[zk: localhost:2181(CONNECTED) 2] ls &#x2F;hadoop-ha
[mycluster]
[zk: localhost:2181(CONNECTED) 3] ls &#x2F;hadoop-ha&#x2F;mycluster
[]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>f)    启动hadoop集群，可在node1到node4这四台服务器上任意位置执行</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 ~]# start-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>node4上</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">node4上：
[zk: localhost:2181(CONNECTED) 5] ls &#x2F;hadoop-ha&#x2F;mycluster
[ActiveBreadCrumb, ActiveStandbyElectorLock]
 [zk: localhost:2181(CONNECTED) 6] get &#x2F;hadoop-ha&#x2F;mycluster&#x2F;ActiveStandbyElectorLock

	myclusternn2node2 �&gt;(�&gt;
cZxid &#x3D; 0x500000007
ctime &#x3D; Tue Dec 31 11:04:24 CST 2019
mZxid &#x3D; 0x500000007
mtime &#x3D; Tue Dec 31 11:04:24 CST 2019
pZxid &#x3D; 0x500000007
cversion &#x3D; 0
dataVersion &#x3D; 0
aclVersion &#x3D; 0
ephemeralOwner &#x3D; 0x26f599f159b0000
dataLength &#x3D; 27
numChildren &#x3D; 0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>node2占用着锁，它的状态是active的</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">1970 Jps
1158 QuorumPeerMain
1335 JournalNode
1546 DataNode
1660 DFSZKFailoverController
1871 NameNode
[root@node2 hadoop]# kill -9 1871
[root@node2 hadoop]# jps
1158 QuorumPeerMain
1335 JournalNode
1546 DataNode
1980 Jps
1660 DFSZKFailoverController<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>​    </p>
<p>node4上继续：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">node4上继续：
[zk: localhost:2181(CONNECTED) 10] get &#x2F;hadoop-ha&#x2F;mycluster&#x2F;ActiveStandbyElectorLock

	myclusternn2node1 �&gt;(�&gt;
cZxid &#x3D; 0x50000000b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>node2访问不了，node1变为active<br>node2上再次启动namenode</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node2 hadoop]# hadoop-daemon.sh start namenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>node2依然为standby。变为备机。</p>
<p>node1上停掉zkfc执行：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 hadoop]# hadoop-daemon.sh stop zkfc
#或
[root@node1 hadoop]# jps
1158 QuorumPeerMain
1335 JournalNode
1546 DataNode
2012 NameNode
1660 DFSZKFailoverController
2111 Jps
[root@node1 hadoop]# kill -9 1660
[root@node1 hadoop]# jps
1158 QuorumPeerMain
1335 JournalNode
2136 Jps
1546 DataNode
2012 NameNode<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>node2变为active状态<br>stop-dfs.sh停止hadoop服务。</p>
<p>node1上编写zk、hdfs启动脚本</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 ~]# vim starthdfs.sh
#!&#x2F;bin&#x2F;bash
for node in node2 node3 node4
do
    ssh $node &quot;source &#x2F;etc&#x2F;profile;zkServer.sh start&quot;
done

sleep 1
start-dfs.sh

:wq

[root@node1 ~]# chmod +x starthdfs.sh
[root@node1 ~]# cp starthdfs.sh stopdfs.sh
[root@node1 ~]# vim stopdfs.sh
#!&#x2F;bin&#x2F;bash
Stop-dfs.sh
sleep 1

for node in node2 node3 node4
do
    ssh $node &quot;source &#x2F;etc&#x2F;profile;zkServer.sh stop&quot;
done


:wq<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<p>如果格式化之后，启动：<br>    启动三台zk<br>    随意节点：start-dfs.sh<br>    hadoop-daemon.sh stop namenode<br>    hadoop-daemon.sh stop zkfc</p>
<h6 id="1-5-2-6-zookeeper操作"><a href="#1-5-2-6-zookeeper操作" class="headerlink" title="1.5.2.6    zookeeper操作"></a>1.5.2.6    zookeeper操作</h6><p>在node2或者node3或者node4上运行</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">zkCli.sh
	
ls &#x2F;hadoop-ha&#x2F;mycluster 查看临时文件 
get &#x2F;hadoop-ha&#x2F;mycluster&#x2F;ActiveStandbyElectorLock 查看临时文件的内容
#退出zkCli.sh
quit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以“hdfs haadmin -help <command>”查看帮助</p>
<p>transitionToActive和transitionToStandby - 将NameNode切换到Active或Standby状态<br>这两个命令不进行围栏操作，最好少用。最好使用“hdfs haadmin -failover”。</p>
<p>failover – 在指定的两个NameNode之间触发一个故障切换<br>如果第一个NameNode处于Standby状态，这个命令简单地让第二个NameNode处于Active状态，不报错。如果第一个处于Active状态，则尝试将它置于Standby状态。如果失败了，则fencing method会执行dfs.ha.fencing.methods列表中的下一个命令，直到有一个执行成功。在这之后才会将第二个NameNode转换为Active状态。如果没有fencing method成功，第二个NameNode不会转换为Active状态，同时报错。</p>
<p>getServiceState – 返回指定的NameNode处于Active或Standby状态<br>连接给定的NameNode并获取它的状态，返回“standby”或“active”到标准输出。这个命令用于定时器作业或监控脚本等需要根据NameNode状态执行不同操作的场合。</p>
<p>getAllServiceState – 返回所有NameNode的状态<br>连接到所有配置的NameNode，在标准输出为每个NameNode打印“standby”或“active”。</p>
<p>checkHealth – 检查给定NameNode的健康状态<br>连接到指定的NameNode并检查其健康状态。NameNode会进行自我诊断，包括检查内部服务是否正常运行。如果NameNode运行正常，则返回0，非0表示运行不正常。一般监控的时候使用。<br>需要注意的是，该命令还没有实现，当前如果不是NameNode宕机，只返回成功。</p>
<h4 id="1-6-java客户端操作HDFS-gt-gt-（python可调用java-api）"><a href="#1-6-java客户端操作HDFS-gt-gt-（python可调用java-api）" class="headerlink" title="1.6    java客户端操作HDFS &gt;&gt; （python可调用java api）"></a>1.6    java客户端操作HDFS &gt;&gt; （python可调用java api）</h4><h5 id="1-6-1-windows上部署hadoop包"><a href="#1-6-1-windows上部署hadoop包" class="headerlink" title="1.6.1    windows上部署hadoop包"></a>1.6.1    windows上部署hadoop包</h5><h5 id="1-6-2-windows环境变量配置"><a href="#1-6-2-windows环境变量配置" class="headerlink" title="1.6.2    windows环境变量配置"></a>1.6.2    windows环境变量配置</h5><h5 id="1-6-3-eclipse插件-了解"><a href="#1-6-3-eclipse插件-了解" class="headerlink" title="1.6.3    eclipse插件(了解)"></a>1.6.3    eclipse插件(了解)</h5><h5 id="1-6-4-IDEA配置hadoop插件"><a href="#1-6-4-IDEA配置hadoop插件" class="headerlink" title="1.6.4    IDEA配置hadoop插件"></a>1.6.4    IDEA配置hadoop插件</h5><h5 id="1-6-5-HDFS-API"><a href="#1-6-5-HDFS-API" class="headerlink" title="1.6.5    HDFS API"></a>1.6.5    HDFS API</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> hdfsimprot <span class="token operator">*</span>

<span class="token comment">#创建连接</span>
client <span class="token operator">=</span> Client<span class="token punctuation">(</span><span class="token string">"http://192.168.40.101:50070/"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>client<span class="token punctuation">.</span>status<span class="token punctuation">(</span><span class="token string">"/data"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment">#创建目录</span>
client<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span><span class="token string">'/data/api'</span><span class="token punctuation">)</span>

<span class="token comment">#删除</span>
client<span class="token punctuation">.</span>delete<span class="token punctuation">(</span><span class="token string">'/data/api'</span><span class="token punctuation">,</span>recursive<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
recursive:flase 被删除的为目录是，只能删除空目录
'''</span>

<span class="token comment">#上传</span>
client<span class="token punctuation">.</span>upload<span class="token punctuation">(</span>hdfs_path<span class="token punctuation">,</span> local_path<span class="token punctuation">,</span> n_threads<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> temp_dir<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>chunk_size<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token number">16</span><span class="token punctuation">,</span> progress<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> cleanup<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>


<span class="token comment">#下载# 有问题  会报错</span>
client<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">"/data/api/hh.txt"</span><span class="token punctuation">,</span><span class="token string">'./.hhh.txt'</span><span class="token punctuation">,</span>overwrite<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="3-MapReduce"><a href="#3-MapReduce" class="headerlink" title="3 MapReduce"></a>3 MapReduce</h3><h4 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h4><p>mapreduce原语（独创）<br>mapreduce工作流程（重点）<br>MR作业提交流程（重点）<br>YARN RM-HA搭建（熟练）<br>运行自带的wordcount（了解）<br>动手写wordcount（熟练）</p>
<h4 id="1-2-MapReduce概述"><a href="#1-2-MapReduce概述" class="headerlink" title="1.2    MapReduce概述"></a>1.2    MapReduce概述</h4><p>map     映射<br>Reduce  汇聚、缩减</p>
<p>目标：普通人都能理解的理论基础<br>比如：如何统计北京一共有多少栋楼房？<br>源数据：<br>Haidian 200   Haidian 230  Haidian 200  Haidian 230<br>Haidian 300   Haidian 330<br>Haidian 400   Haidian 420<br>Haidian 500   Haidian 540<br>Haidian 600   Haidian 120 </p>
<p>Map-&gt;<br>Haidian  200<br>Haidian 230<br>Haidian  200<br>Haidian   200,300,400,230,600</p>
<p>Reducer:<br> Haidian  sum<br><a target="_blank" rel="noopener" href="https://ibb.co/VNPqJbP">工作流程</a><br><strong>原语</strong>:&lt;&lt;相同的key为一组,这一组数据调用一次reduce方法,方法内迭代计算这一组数据&gt;&gt;<br>Map-Reduce: 线性依赖关系,先执行完map,再执行reduce</p>
<p>MapTask:<br>    映射:保证原语中组的实现<br>    并行度:split<br>        split:大小可调整,默认等于hdfs中block的大小<br>        框架默认,hdfs一个文件多少个block,就会有多少个map<br>    计算级别:split中一条记录(record)调用一次map方法!</p>
<p>ReduceTask:<br>    汇聚:<br>    计算级别:按照组group为单位,一组调用一次reduce方法!<br>    并行度:<br>        理想状态:多少组group对应多少个reduceTask<br>但是,其实一个reduceTask可以线性处理若干组<br>术语对比关系:<br>术语对比关系:<br>•    block &gt; split<br>    –    1:1（默认）<br>    –    N:1 (运算量较少的时候)<br>    –    1:N (运算量较大的时候)<br>•    split &gt; map<br>    –    1:1<br>•    map &gt; reduce<br>    –    N:1<br>    –    M:N<br>    –    1:1<br>    –    1:N<br>•    group(key)&gt;partition（redues task）     reduce(){}<br>    –    1:1<br>    –    N:1<br>    –    M:N<br>    –    1:N?  &gt;违背了原语<br>•    partition &gt; outputfile</p>
<p>1、每个block会有map任务<br>2、block切分为切片，每个切片对应一个map任务，默认一个block一个切片，一个map<br>3、map默认按行读取切片数据，组成键值对&lt;当前行字节偏移量, “读到的行字符串”&gt;<br>4、map函数对该键值对进行计算，输出若干键值对。&lt;key, value, partition&gt;<br>    partition指定该键值对由哪个reducer进行处理<br>5、map输出的kvp写到环形缓冲区，环形缓冲区默认100MB，阈值80%，当环缓达到80%就向磁盘溢写小文件，该小文件首先按照分区号排序，相同分区号的按key进行排序。<br>6、默认如果落磁盘的小文件达到了3个，则进行归并，归并的大文件也是按分区号排序，相同分区号按照key进行排序。只是一个归并。<br>7、如果map任务处理完了，它的输出被下载到reducer所在主机<br>    按照HTTP GET的方式下载到reducer：<br>    reducer发送HTTP GET请求到mapper主机下载数据，该过程是洗牌shuffle<br>8、每个map任务都要经历运行结束洗牌的过程<br>9、可以设置combinClass，先在map端对数据进行一个压缩，比如10w个&lt;hello,1&gt;压缩为1个&lt;hello, 10w&gt;通过网络IO洗牌，肯定要快很多。一般情况下，combineClass就是一个reducerClass</p>
<h4 id="1-3-mapreduce工作流程"><a href="#1-3-mapreduce工作流程" class="headerlink" title="1.3    mapreduce工作流程"></a>1.3    mapreduce工作流程</h4><p>官方给的定义：系统执行排序、将map输出作为输入传给reducer的过程称为Shuffle。（看完是不是一脸懵逼）通俗来讲，就是从map产生输出开始到reduce消化输入的整个过程称为Shuffle。如下图用黑线框出的部分：<br><a target="_blank" rel="noopener" href="https://ibb.co/bNHHgSs">展示</a><br><strong>圆形缓冲区介绍</strong><br><a target="_blank" rel="noopener" href="https://ibb.co/K61mT99">缓冲</a><br>每一个map任务都会有一个圆形缓冲区。默认大小100MB（io.sort.mb属性）阈值0.8也就是80MB**(mapreduce.map.sort.spill.percent属性指定）**</p>
<p><a target="_blank" rel="noopener" href="https://ibb.co/wzwjZfG">缓冲</a></p>
<p>一旦达到阈值一个后台线程开始把内容写到(spill)磁盘的指定目录**[mapred.local.dir]**下的新建的一个溢出写文件。写入磁盘前先partition、sort、[combiner]。一个map task任务可能产生N个磁盘文件。map task运算完之后，产生了N个文件，然后将这些文件merge合成一个文件。<br>如果N=2，合成的新文件写入磁盘前只经过patition（分区）和sort（排序）过程，不会执行combiner合并（无论是否指定combiner类），如下图所示：<br><a target="_blank" rel="noopener" href="https://ibb.co/JnHz49q">图示</a></p>
<p>如果N&gt;=3，合成的新文件写入磁盘前经过patition（分区）、sort（排序）过和combiner合并（前提是指定了combiner类），如下图所示：<br><a target="_blank" rel="noopener" href="https://ibb.co/G5g8D4K">图示</a></p>
<p><strong>Q:思考：为什么只有当N&gt;=3时，合成文件才会执行combiner呢？</strong><br>A:这是因为如果N&lt;3时，执行combiner虽然减少了文件的大小，但是同时产生了一定的系统开销。由于减少的文件大小不大，权衡利弊后，确定N&lt;2时不在执行combiner操作。当该map task全部执行完之后，对应的reduce task将会拷贝对应分区的数据（该过程称为fetch），如下图所示：<br><a target="_blank" rel="noopener" href="https://ibb.co/58Zt9MK">图示</a><br>其它的map task任务完成后，对应的reduce task也同样执行fetch操作，如下图所示：<br><a target="_blank" rel="noopener" href="https://ibb.co/b7GZC9Q">图示</a></p>
<p>每个map任务的完成时间可能不同，因此只要有一个任务完成，reduce任务就开始复制其输出。该阶段被称为reduce的复制阶段。reduce任务有少量复制线程，因此能够并行取得map输出。默认值是5个线程，但这个默认值可以通过设置**[mapred.reduce.parallel.copies]**属性改变。<br><a target="_blank" rel="noopener" href="https://ibb.co/gv1V8n3">图示</a></p>
<p>复制完所有map输出后，reduce任务进入合并阶段，该阶段将合并map输出，并维持其顺序排序（相当于执行了sort），如果指定了combiner，在写入磁盘前还会执行combiner操作。</p>
<p><strong>那么具体是如何合并的呢？</strong><br>合并因子默认是10，可以通过io.sort.factor属性设置。合并过程是循环进行了，可能叫经过多趟合并。目标是合并最小数量的文件以便满足最后一趟的合并系数。假设有40个文件，我们不会在四趟中每趟合并10个文件从而得到4个文件。相反，第一趟只合并4个文件，随后的三趟分别合并10个文件。再最后一趟中4个已合并的文件和余下的6个（未合并的）文件合计10个文件。具体流程如下图所示：<br><a target="_blank" rel="noopener" href="https://ibb.co/cDSgf5d">图示</a></p>
<p>注意：这并没有改变合并次数，它只是一个优化措施，目的是尽量减少写到磁盘的数据量，因为最后一趟总是直接合并到reduce。<br>看到这里您是否理解了Shuffle的具体原理呢，如果没有，也没有关系，接下来我们通过一个wordcount案例再将整个流程梳理一遍。首先map任务的代码如下:</p>
<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">cn<span class="token punctuation">.</span>geekmooc</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IOException</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">LongWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Mapper</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WCMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">LongWritable</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> <span class="token punctuation">&#123;</span>
	<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span><span class="token class-name">LongWritable</span> ikey<span class="token punctuation">,</span> <span class="token class-name">Text</span> ivalue<span class="token punctuation">,</span> <span class="token class-name">Context</span> context<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">&#123;</span>
		<span class="token class-name">String</span> line <span class="token operator">=</span> ivalue<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token class-name">String</span> words<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">String</span> word <span class="token operator">:</span> words<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
			context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">LongWritable</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
	<span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><a target="_blank" rel="noopener" href="https://ibb.co/8NRk5Nm">分区</a><br>在分区（分区规则：按首字母分四个区，分别为a-i,j-q,r-z,其它）的过程中，会将相同的单词合并到一起，将出现次数用逗号隔开，如上图所示。注意此时还没有排序。分区代码如下：</p>
<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">cn<span class="token punctuation">.</span>geekmooc</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">LongWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Partitioner</span><span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WCPatitioner</span> <span class="token keyword">extends</span> <span class="token class-name">Partitioner</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> <span class="token punctuation">&#123;</span>
	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">getPartition</span><span class="token punctuation">(</span><span class="token class-name">Text</span> key<span class="token punctuation">,</span> <span class="token class-name">LongWritable</span> value<span class="token punctuation">,</span> <span class="token keyword">int</span> numPartitions<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
		<span class="token keyword">int</span> first_char <span class="token operator">=</span> key<span class="token punctuation">.</span><span class="token function">charAt</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">if</span><span class="token punctuation">(</span>first_char<span class="token operator">>=</span><span class="token number">97</span><span class="token operator">&amp;&amp;</span>first_char<span class="token operator">&lt;=</span><span class="token number">105</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span><span class="token comment">//a- j</span>
			<span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span><span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span>first_char<span class="token operator">>=</span><span class="token number">106</span><span class="token operator">&amp;&amp;</span>first_char<span class="token operator">&lt;=</span><span class="token number">113</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span><span class="token comment">//k-q</span>
			<span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span><span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span>first_char<span class="token operator">>=</span><span class="token number">114</span><span class="token operator">&amp;&amp;</span>first_char<span class="token operator">&lt;=</span><span class="token number">122</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span><span class="token comment">//r-  z</span>
			<span class="token keyword">return</span> <span class="token number">2</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span><span class="token keyword">else</span><span class="token punctuation">&#123;</span>
			<span class="token keyword">return</span> <span class="token number">3</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
	<span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>接着执行排序操作，默认排序规则是按照key的字典升序排序，当然你也可以指定排序规则，排序后如下图所示<br><a target="_blank" rel="noopener" href="https://ibb.co/j4STHVq">图示</a></p>
<p>接下来执行combiner操作，将每个单词后续的1求和，WCCombiner类代码如下:</p>
<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">cn<span class="token punctuation">.</span>tedu</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IOException</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Iterator</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">LongWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Reducer</span><span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WCCombiner</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Text</span><span class="token punctuation">,</span>
<span class="token class-name">LongWritable</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span><span class="token punctuation">&#123;</span>
	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span><span class="token class-name">Text</span> key<span class="token punctuation">,</span> <span class="token class-name">Iterable</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> values<span class="token punctuation">,</span>
			<span class="token class-name">Reducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span><span class="token punctuation">.</span>Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">&#123;</span>
		 <span class="token class-name">Iterator</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> iter <span class="token operator">=</span> values<span class="token punctuation">.</span><span class="token function">iterator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		 <span class="token keyword">long</span> count <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
		 <span class="token keyword">while</span><span class="token punctuation">(</span>iter<span class="token punctuation">.</span><span class="token function">hasNext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
			 count <span class="token operator">+=</span> iter<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		 <span class="token punctuation">&#125;</span>
		 context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">LongWritable</span><span class="token punctuation">(</span>count<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://ibb.co/C2cfY0h">结果</a><br>combiner的结果如上图所示<br>map任务执行完，产生N个spill文件，接着对N个文件进行合并，分以下两种情况：1.N&lt;3，无论是否指定combiner类，合并文件时都不会执行combiner<br><a target="_blank" rel="noopener" href="https://ibb.co/Rhjxg0G">图示</a></p>
<p>2.N&gt;=3,如果指定了combiner类将执行combiner操作，如下图：<br><a target="_blank" rel="noopener" href="https://ibb.co/6mL5Ddz">combiner操作</a></p>
<p>接下来进入fetch（或copy）阶段<br><a target="_blank" rel="noopener" href="https://ibb.co/RhLmbLP">fetch阶段</a></p>
<p>然后在reduce端进行合并<br>然后执行最后一趟合并，并将结果直接传给reduce<br><a target="_blank" rel="noopener" href="https://ibb.co/3hYJVxH">结果</a><br>reduce类代码如下：</p>
<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">cn<span class="token punctuation">.</span>geekmooc</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IOException</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">LongWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Reducer</span><span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WCReducer</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> <span class="token punctuation">&#123;</span>
	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span><span class="token class-name">Text</span> key<span class="token punctuation">,</span> <span class="token class-name">Iterable</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> values<span class="token punctuation">,</span>
			<span class="token class-name">Reducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span><span class="token punctuation">.</span>Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">&#123;</span>
		<span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>key<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">":"</span><span class="token operator">+</span>values<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">long</span> count <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
		<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">LongWritable</span> val <span class="token operator">:</span> values<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
			count <span class="token operator">+=</span> val<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">LongWritable</span><span class="token punctuation">(</span>count<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>reduce task执行后，输出结果：<br><a target="_blank" rel="noopener" href="https://ibb.co/HCd6QX4">结果</a></p>
<h4 id="1-1-Hadoop-1-x（了解）"><a href="#1-1-Hadoop-1-x（了解）" class="headerlink" title="1.1 Hadoop 1.x（了解）"></a>1.1 Hadoop 1.x（了解）</h4><h5 id="1-4-1-架构"><a href="#1-4-1-架构" class="headerlink" title="1.4.1    架构"></a>1.4.1    架构</h5><p><a target="_blank" rel="noopener" href="https://ibb.co/pXjX9rD">架构</a></p>
<p>体现计算向数据移动<br><a target="_blank" rel="noopener" href="https://ibb.co/qyD9LzS">data_map</a><br><a target="_blank" rel="noopener" href="https://ibb.co/TPwsz4W">data_reduce</a></p>
<p>•    MRv1角色：<br>    –    JobTracker<br>        •    核心，主，单点<br>        •    调度所有的作业<br>        •    监控整个集群的资源负载<br>    –    TaskTracker<br>        •    从自身节点资源管理<br>        •    和JobTracker心跳，汇报资源，获取Task<br>    –    Client<br>        •    作业为单位<br>        •    规划作业计算分布<br>        •    提交作业资源到HDFS<br>        •    最终提交作业到JobTracker</p>
<p>•    弊端：<br>    –    JobTracker：负载过重，单点故障<br>    –    资源管理与计算调度强耦合，其他计算框架需要重复实现资源管理<br>    –    不同框架对资源不能全局管理</p>
<h5 id="1-4-2-MR执行流程（扩展阅读）"><a href="#1-4-2-MR执行流程（扩展阅读）" class="headerlink" title="1.4.2    MR执行流程（扩展阅读）"></a>1.4.2    MR执行流程（扩展阅读）</h5><p>一、4个独立的实体：<br>    1.    客户端：提交MapReduce作业<br>    2.    jobtracker：协调作业的运行，它是一个Java应用程序，它的主类时JobTracker<br>    3.    tasktracker:运行作业划分后的任务，它是Java应用程序，它的主类时TaskTracker。<br>    4.    HDFS：分布式文件系统，用来在其他实体间共享作业文件。</p>
<p>二、具体执行流程：<br><a target="_blank" rel="noopener" href="https://ibb.co/NSbrhk0">执行流程</a></p>
<ol>
<li>客户端提交一个mr的jar包给JobClient(提交方式：hadoop jar …)</li>
<li>   JobClient通过RPC和JobTracker）进行通信，返回一个存放jar包的地址（HDFS）和jobId</li>
<li><pre><code>   client将运行作业所需要的资源（包括JAR文件、配置文件和计算所得的输入分片）复制到HDFS中的以作业id命名的目录下(path = hdfs上的地址 + jobId) 
</code></pre>
</li>
<li><pre><code>   开始提交任务(任务的描述信息，不是jar, 包括jobid，jar存放的位置，配置信息等等) 
</code></pre>
</li>
<li><pre><code>   JobTracker进行初始化任务
</code></pre>
</li>
<li>   读取HDFS上的要处理的文件，开始计算输入分片，每一个分片对应一个MapperTask</li>
<li>   TaskTracker通过心跳机制领取任务（任务的描述信息）</li>
<li>   下载所需的jar，配置文件等</li>
<li>   TaskTracker启动一个java child子进程，</li>
<li>   用来执行具体的任务（MapperTask或ReducerTask）将结果写入到HDFS当中</li>
</ol>
<h4 id="1-5-Hadoop2-x"><a href="#1-5-Hadoop2-x" class="headerlink" title="1.5    Hadoop2.x"></a>1.5    Hadoop2.x</h4><h5 id="1-5-1-架构"><a href="#1-5-1-架构" class="headerlink" title="1.5.1    架构"></a>1.5.1    架构</h5><p><a target="_blank" rel="noopener" href="https://ibb.co/NycGnwn">架构</a><br>•    MRv2：On YARN<br>    –    YARN：解耦资源与计算<br>        •    ResourceManager<br>            –    主，核心<br>            –    集群节点资源管理<br>        •    NodeManager<br>            –    与RM汇报资源<br>            –    管理Container生命周期<br>            –    计算框架中的角色都以Container表示<br>        •    Container：【节点NM，CPU,MEM,I/O大小，启动命令】<br>            –    默认NodeManager启动线程监控Container大小，超出申请资源额度，kill<br>            –    支持Linux内核的Cgroup<br>    –    MR ：<br>        •    MR-ApplicationMaster-Container<br>            –    作业为单位，避免单点故障，负载到不同的节点<br>            –    创建Task需要和RM申请资源（Container）<br>        •    Task-Container<br>    –    Client：<br>        •    RM-Client：请求资源创建AM<br>        •    AM-Client：与AM交互</p>
<p>YARN<br>    –    YARN：Yet Another Resource Negotiator；<br>    –    Hadoop 2.0新引入的资源管理系统，直接从MRv1演化而来的；<br>        –    核心思想：将MRv1中JobTracker的资源管理和任务调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现<br>        –    ResourceManager：负责整个集群的资源管理和调度<br>        –    ApplicationMaster：负责应用程序相关的事务，比如任务调度、任务监控和容错等<br>    –    YARN的引入，使得多个计算可运行在一个集群中<br>        –    每个job对应一个ApplicationMaster<br>        –    目前多个计算框架可以运行在YARN上，比如MapReduce、Spark、Storm等</p>
<p>MapReduce  On  YARN：MRv2<br>–    将MapReduce作业直接运行在YARN上，而不是由JobTracker和TaskTracker构建的MRv1系统中<br>–    基本功能模块<br>    –    YARN：负责资源管理和调度<br>    –    MRAppMaster：负责任务切分、任务调度、任务监控和容错等<br>    –    MapTask/ReduceTask：任务驱动引擎，与MRv1一致</p>
<p>–    每个MapRduce作业对应一个MRAppMaster<br>    –    MRAppMaster任务调度<br>    –    YARN将资源分配给MRAppMaster<br>    –    MRAppMaster进一步将资源分配给内部的任务</p>
<p>–    MRAppMaster容错<br>    –    失败后，由YARN重新启动<br>    –    任务失败后，MRAppMaster重新申请资源<br>    <a target="_blank" rel="noopener" href="https://ibb.co/mcQ0MWK">容错</a></p>
<h5 id="1-5-2-MR执行流程"><a href="#1-5-2-MR执行流程" class="headerlink" title="1.5.2    MR执行流程"></a>1.5.2    MR执行流程</h5><p><a target="_blank" rel="noopener" href="https://ibb.co/xLZpjsx">执行流程</a><br>一．    作业提交：</p>
<ol>
<li>   提交作业job后，job.waitForCompletion（true）调用monitorAndPrintJob()方法每秒轮询作业进度，如果发现自上次报告后有改变，便把进度报告给控制台。Job的submit()方法创建一个内部的JobSubmitter实例，并调用其submitJobInternal方法（步骤1）。作业完成后，如果成功，就显示计数器；如果失败，这将导致作业失败的错误记录到控制台。</li>
</ol>
<p>JobSubmitter所实现的作业提交过程如下所述：<br>2.    向资ResourceManager源管理器请求一个新作业的ID，用于MapReduce作业ID。<br>3.    作业客户端检查作业的输出说明，计算输入分片splits并将作业资源（包括作业Jar包、配置文件和分片信息）复制到HDFS<br>4.    通过调用资源管理器上的submitApplication（）方法提交作业</p>
<p>二．    作业初始化<br>5.    资源管理器ResourceManager收到调用他的submitApplication（）消息后，便将请求传递给调度器（scheduler）。调度器分配一个容器（Container），然后资源管理器在节点管理器（NodeManager）的管理下载容器中启动应用程序的master进程（步骤5a和5b）<br>6.    MapReduce作业的application master是一个Java应用程序，它的主类是MRAppMaster。它对作业进行初始化：通过创建多个簿记对象以保持对作业进度的跟踪，因为它将接受来自任务的进度和完成报告（步骤6）。</p>
<ol start="7">
<li>   接下来，它接受来自共享文件系统的在客户端计算的输入分片（步骤7）。对每一个分片创建一个map任务对象以及由mapreduce. job.reduces属性确定的多个reduce任务对象。</li>
</ol>
<p>三．    任务分配<br>8.    AppMaster为该作业中的所有map任务和reduce任务向资源管理器请求容器。</p>
<p>四．    任务执行<br>9.    一旦资源管理器的调度器为任务分配了容器，AppMaster就通过与节点管理器NodeManager通讯来启动容器（步骤9a和9b）。<br>10.    该任务由主类为YarnChild的Java应用程序执行。在它允许任务之前，首先将任务需要的资源本地化，包括作业的配置、JAR文件和所有来自分布式缓存的文件.<br>11.    最后运行map任务或reduce任务。</p>
<p>五．    进度和状态更新<br>在YARN下运行时，任务每3秒钟通过umbilical接口向APPMaster汇报进度和状态。客户端每一秒钟（通过mapreduce.client.<br>Progressmonitor.pollinterval设置）查询一次AppMaster以接收进度更新，通常都会向用户显示。</p>
<p>六．    作业完成<br>除了向AppMaster查询进度外，客户端每5秒还通过调用Job的waitForCompletion()来检测作业是否完成。查询的间隔可以通过mapreduce.client.completion.pollinterval属性进行设置。作业完成后，AppMaster和任务容器清理器工作状态。</p>
<h4 id="1-6-YARN-RM-HA搭建"><a href="#1-6-YARN-RM-HA搭建" class="headerlink" title="1.6    YARN RM-HA搭建"></a>1.6    YARN RM-HA搭建</h4><p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r2.6.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">高可用</a></p>
<p><a target="_blank" rel="noopener" href="https://ibb.co/FYzy7T2">rm-ha</a></p>
<h5 id="1-6-1-mapred-site-xml"><a href="#1-6-1-mapred-site-xml" class="headerlink" title="1.6.1    mapred-site.xml"></a>1.6.1    mapred-site.xml</h5><p>local/classic/yarn<br>指定mr作业运行的框架：要么本地运行，要么使用MRv1，要么使用yarn</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="1-6-2-yarn-site-xml"><a href="#1-6-2-yarn-site-xml" class="headerlink" title="1.6.2    yarn-site.xml"></a>1.6.2    yarn-site.xml</h5><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token comment">&lt;!-- 让yarn的容器支持mapreduce的洗牌，开启shuffle服务 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 启用resourcemanager的HA --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.ha.enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 指定zookeeper集群的各个节点地址和端口号 --></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.zk-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node2:2181,node3:2181,node4:2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment">&lt;!-- 标识集群，以确保 RM 不会接管另一个集群的活动。 --></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.cluster-id<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>cluster1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- RM HA的两个resourcemanager的名字 --></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.ha.rm-ids<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>rm1,rm2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment">&lt;!-- 指定rm1的reourcemanager进程所在的主机名称 --></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname.rm1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 指定rm2的reourcemanager进程所在的主机名称 --></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname.rm2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>node4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>将配置文件在四台服务器同步</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 hadoop]# pwd
&#x2F;opt&#x2F;hadoop-2.6.5&#x2F;etc&#x2F;hadoop
[root@node1 hadoop]scp mapred-site.xml yarn-site.xml  node[234]:&#96;pwd&#96;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><strong>node1:</strong><br>首先启动HDFS<br>start-ha.sh</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#!&#x2F;bin&#x2F;bash
for node in node2 node3 node4
do
   ssh $node &quot;source &#x2F;etc&#x2F;profile; zkServer.sh start&quot;
done

sleep 1

start-dfs.sh

echo &quot;--------------node1-jps----------------&quot;
jps

for node in node2 node3 node4
do
  echo &quot;---------------$node-jps-------------------&quot;
  ssh $node &quot;source &#x2F;etc&#x2F;profile; jps&quot;
done
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>在node3和node4上执行命令,启动ResourceManager：<br>Node3:<br>start-yarn.sh（只能启动本机上的ResourceManager和其他节点的NodeManager）<br>Node4:<br>yarn-daemon.sh  start  resourcemanager</p>
<p><a target="_blank" rel="noopener" href="http://node3:8088/">http://node3:8088</a><br><a target="_blank" rel="noopener" href="https://ibb.co/BsL5kpG">结果</a></p>
<p><a target="_blank" rel="noopener" href="http://node4:8088/cluster/cluster">http://node4:8088/cluster/cluster</a><br><a target="_blank" rel="noopener" href="https://ibb.co/WBsr3cP">结果</a></p>
<p><strong>高可用演示：</strong><br>node3上执行：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">yarn-deamon.sh  stop  resourcemanager<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><a target="_blank" rel="noopener" href="http://node4:8088/cluster/cluster%E5%8F%98%E4%B8%BAactive%E7%9A%84%E3%80%82">http://node4:8088/cluster/cluster变为active的。</a><br><a target="_blank" rel="noopener" href="https://ibb.co/sF8HB2Z">图示</a></p>
<h4 id="1-7-运行自带的wordcount"><a href="#1-7-运行自带的wordcount" class="headerlink" title="1.7    运行自带的wordcount"></a>1.7    运行自带的wordcount</h4><h5 id="1-7-1-运行的命令："><a href="#1-7-1-运行的命令：" class="headerlink" title="1.7.1    运行的命令："></a>1.7.1    运行的命令：</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">root@node1 ~]# vim wc.txt
hello  tom
andy  joy
hello  rose
hello  joy
mark  andy
hello  tom
andy  rose
hello  joy
[root@node1 ~]# hdfs dfs -mkdir &#x2F;input
[root@node1 ~]# hdfs dfs -put wc.txt &#x2F;input
[root@node1 ~]# hdfs dfs -ls &#x2F;input
-rw-r--r--   2 root supergroup         88 2020-02-23 22:28 &#x2F;input&#x2F;wc.txt
[root@node1 ~]# cd &#x2F;opt&#x2F;hadoop-2.6.5&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;
[root@node1 mapreduce]#hadoop jar hadoop-mapreduce-examples-2.6.5.jar wordcount  &#x2F;input  &#x2F;output
[root@node1 mapreduce]# hdfs dfs -ls &#x2F;output
-rw-r--r--   2 root supergroup          0 2020-02-23 22:31 &#x2F;output&#x2F;_SUCCESS
-rw-r--r--   2 root supergroup         41 2020-02-23 22:31 &#x2F;output&#x2F;part-r-00000
[root@node1 mapreduce]# cd
[root@node1 ~]#hdfs dfs -get &#x2F;output&#x2F;part-r-00000
[root@node1 ~]#ls
[root@node1 ~]#cat part-r-00000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>*input:是hdfs文件系统中数据所在的目录<br>*ouput:是hdfs中不存在的目录，mr程序运行的结果会输出到该目录</p>
<h5 id="1-7-2-输出目录内容："><a href="#1-7-2-输出目录内容：" class="headerlink" title="1.7.2    输出目录内容："></a>1.7.2    输出目录内容：</h5><p>-rw-r–r–   3 root supergroup          0 2017-07-02 02:49 /mr/test/output/_SUCCESS<br>-rw-r–r–   3 root supergroup         49 2017-07-02 02:49 /mr/test/output/part-r-00000<br>/_SUCCESS：是信号/标志文件<br>/part-r-00000：是reduce输出的数据文件<br>r：reduce的意思，00000是对应的reduce编号，多个reduce会有多个数据文件</p>
<h5 id="1-7-3-启动脚本和停止脚本："><a href="#1-7-3-启动脚本和停止脚本：" class="headerlink" title="1.7.3    启动脚本和停止脚本："></a>1.7.3    启动脚本和停止脚本：</h5><p>start-hdfs-ha-rm-ha.sh</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#!&#x2F;bin&#x2F;bash
for node in node2 node3 node4
do
   ssh $node &quot;source &#x2F;etc&#x2F;profile; zkServer.sh start&quot;
done

sleep 1

start-dfs.sh

ssh node3 &quot;. &#x2F;etc&#x2F;profile; start-yarn.sh&quot;
ssh node4 &quot;. &#x2F;etc&#x2F;profile; yarn-daemon.sh start resourcemanager&quot;

echo &quot;--------------node1-jps----------------&quot;
jps

for node in node2 node3 node4
do
  echo &quot;---------------$node-jps-------------------&quot;
  ssh $node &quot;source &#x2F;etc&#x2F;profile; jps&quot;
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>stop-hdfs-ha-rm-ha.sh</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#!&#x2F;bin&#x2F;bash

ssh node4 &quot;. &#x2F;etc&#x2F;profile; stop-yarn.sh&quot;
ssh node3 &quot;. &#x2F;etc&#x2F;profile; yarn-daemon.sh stop resourcemanager&quot;

stop-dfs.sh

for node in node2 node3 node4
do
  ssh $node &quot;source &#x2F;etc&#x2F;profile; zkServer.sh stop&quot;
done

echo &quot;-------------node1-jps-----------------&quot;
jps

for node in node2 node3 node4
do
  echo &quot;---------------$node-jps-----------------&quot;
  ssh $node &quot;source &#x2F;etc&#x2F;profile; jps&quot;
done
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="1-8-动手写wordcount"><a href="#1-8-动手写wordcount" class="headerlink" title="1.8    动手写wordcount"></a>1.8    动手写wordcount</h4><h5 id="1-8-1-新建java项目"><a href="#1-8-1-新建java项目" class="headerlink" title="1.8.1    新建java项目"></a>1.8.1    新建java项目</h5><h5 id="1-8-2-添加hadoop的jar包依赖"><a href="#1-8-2-添加hadoop的jar包依赖" class="headerlink" title="1.8.2    添加hadoop的jar包依赖"></a>1.8.2    添加hadoop的jar包依赖</h5><p>121个jar包<br>$HADOOP_HOME/share/hadoop/{common,common/lib,hdfs,hdfs/lib,mapreduce,mapreduce/lib,tools/lib,yarn,yarn/lib}.jar</p>
<h5 id="1-8-3-添加hadoop的配置文件到类路径"><a href="#1-8-3-添加hadoop的配置文件到类路径" class="headerlink" title="1.8.3    添加hadoop的配置文件到类路径"></a>1.8.3    添加hadoop的配置文件到类路径</h5><p>从集群拷贝这四个文件到当前项目类路径<br>core-site.xml<br>hdfs-site.xml<br>mapred-site.xml<br>yarn-site.xml</p>
<h5 id="1-8-4-编写Mapper、Reducer以及MainClass"><a href="#1-8-4-编写Mapper、Reducer以及MainClass" class="headerlink" title="1.8.4    编写Mapper、Reducer以及MainClass"></a>1.8.4    编写Mapper、Reducer以及MainClass</h5><h6 id="1-8-4-1-WCMapper-java"><a href="#1-8-4-1-WCMapper-java" class="headerlink" title="1.8.4.1    WCMapper.java"></a>1.8.4.1    WCMapper.java</h6><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>bjsxt<span class="token punctuation">.</span>mr<span class="token punctuation">.</span>wordcount</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IOException</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">LongWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Mapper</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WCMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">LongWritable</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> <span class="token punctuation">&#123;</span>
	
	<span class="token keyword">private</span> <span class="token class-name">Text</span> outKey <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">private</span> <span class="token class-name">LongWritable</span> outValue <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LongWritable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	
	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span><span class="token class-name">LongWritable</span> key<span class="token punctuation">,</span> <span class="token class-name">Text</span> value<span class="token punctuation">,</span> <span class="token class-name">Context</span> context<span class="token punctuation">)</span>
			<span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">&#123;</span>
		<span class="token comment">//一句话  hello bjsxt 1</span>
		<span class="token class-name">String</span> line <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">//将一句话按照空格隔开为单个单词</span>
		<span class="token comment">// &#123;"hello", "bjsxt", "1"&#125;</span>
		<span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> words <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">String</span> word <span class="token operator">:</span> words<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
			outKey<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">;</span>
			outValue<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token comment">// &lt;"hello", 1></span>
			<span class="token comment">// &lt;"bjsxt", 1></span>
			<span class="token comment">// &lt;"1", 1></span>
			context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>outKey<span class="token punctuation">,</span> outValue<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		
	<span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h6 id="1-8-4-2-WCReducer-java"><a href="#1-8-4-2-WCReducer-java" class="headerlink" title="1.8.4.2    WCReducer.java"></a>1.8.4.2    WCReducer.java</h6><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>bjsxt<span class="token punctuation">.</span>mr<span class="token punctuation">.</span>wordcount</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IOException</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Iterator</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">LongWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Reducer</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WCReducer</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> <span class="token punctuation">&#123;</span>
	
	<span class="token keyword">private</span> <span class="token class-name">LongWritable</span> outValue <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LongWritable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	
	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span><span class="token class-name">Text</span> key<span class="token punctuation">,</span> <span class="token class-name">Iterable</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> values<span class="token punctuation">,</span> <span class="token class-name">Context</span> context<span class="token punctuation">)</span>
			<span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">&#123;</span>
		<span class="token comment">// key表示的单词出现的次数，总数</span>
		<span class="token keyword">long</span> sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
		<span class="token comment">// 获取values的迭代器，用于遍历</span>
		<span class="token class-name">Iterator</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">LongWritable</span><span class="token punctuation">></span></span> itera <span class="token operator">=</span> values<span class="token punctuation">.</span><span class="token function">iterator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">//&lt;"zhangsan", 1></span>
		<span class="token comment">//&lt;"zhangsan-0", 1></span>
		<span class="token comment">//&lt;"zhangsan-1", 1></span>
		<span class="token comment">//&lt;"zhangsan-2", 1></span>
		<span class="token comment">//&lt;"zhangsan-3", 1></span>
		<span class="token comment">//&lt;"zhangsan-4", 1></span>
		
		<span class="token keyword">while</span> <span class="token punctuation">(</span>itera<span class="token punctuation">.</span><span class="token function">hasNext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
			<span class="token comment">// 获取该值</span>
			<span class="token class-name">LongWritable</span> val <span class="token operator">=</span> itera<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token comment">// 将该值转换为long类型</span>
			<span class="token keyword">long</span> num <span class="token operator">=</span> val<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token comment">// 逐个求和</span>
			sum <span class="token operator">+=</span> num<span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		<span class="token comment">// 将总数封装为LongWritable类型对象</span>
		outValue<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 输出到HDFS</span>
		context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> outValue<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token punctuation">&#125;</span>
	
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h6 id="1-8-4-3-MainClass-java"><a href="#1-8-4-3-MainClass-java" class="headerlink" title="1.8.4.3    MainClass.java"></a>1.8.4.3    MainClass.java</h6><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>bjsxt<span class="token punctuation">.</span>mr<span class="token punctuation">.</span>wordcount</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span></span><span class="token class-name">Configuration</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span></span><span class="token class-name">Path</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">LongWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Job</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">JobContext</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span></span><span class="token class-name">FileInputFormat</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span></span><span class="token class-name">FileOutputFormat</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MainClass</span> <span class="token punctuation">&#123;</span>
	
	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">&#123;</span>
		
		<span class="token keyword">if</span> <span class="token punctuation">(</span>args <span class="token operator">==</span> <span class="token keyword">null</span> <span class="token operator">||</span> args<span class="token punctuation">.</span>length <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
			<span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Usage : yarn jar wc.jar com.bjsxt.mr.wordcount.MainClass &lt;input path> &lt;output path>"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token class-name">System</span><span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		
		<span class="token class-name">Configuration</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token class-name">Job</span> job <span class="token operator">=</span> <span class="token class-name">Job</span><span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">//设置主入口程序</span>
		job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span><span class="token class-name">MainClass</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 设置作业名称，该名称可以在UI上看到</span>
		job<span class="token punctuation">.</span><span class="token function">setJobName</span><span class="token punctuation">(</span><span class="token string">"我的数单词"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
<span class="token comment">//		Path inputPath = new Path("/mr/wc/input/hello.txt");</span>
		<span class="token class-name">Path</span> inputPath <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">//设置输入路径</span>
		<span class="token class-name">FileInputFormat</span><span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> inputPath<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
<span class="token comment">//		Path outputPath = new Path("/mr/wc/output");</span>
		<span class="token class-name">Path</span> outputPath <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">//指定输出路径，该路径一定不能存在</span>
		<span class="token class-name">FileOutputFormat</span><span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> outputPath<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">//指定mapper类</span>
		job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span><span class="token class-name">WCMapper</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">//指定reducer类</span>
		job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span><span class="token class-name">WCReducer</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">//map输出键值对的key类型</span>
		job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span><span class="token class-name">Text</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">//map端输出键值对的value类型</span>
		job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span><span class="token class-name">LongWritable</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">//提交作业</span>
		job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
	
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="1-8-5-本地运行测试"><a href="#1-8-5-本地运行测试" class="headerlink" title="1.8.5    本地运行测试"></a>1.8.5    本地运行测试</h5><p>如果想本地运行，则可以如此设置：<br>略》》》</p>
<h5 id="1-8-6-打包"><a href="#1-8-6-打包" class="headerlink" title="1.8.6    打包"></a>1.8.6    打包</h5><p>只打包三个类就可以。<br>略》》》</p>
<h5 id="1-8-7-上传"><a href="#1-8-7-上传" class="headerlink" title="1.8.7    上传"></a>1.8.7    上传</h5><h5 id="1-8-8-运行"><a href="#1-8-8-运行" class="headerlink" title="1.8.8    运行"></a>1.8.8    运行</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@node1 ~]# yarn jar wc.jar com.bjsxt.mr.wordcount.MainClass
Usage : yarn jar wc.jar com.bjsxt.mr.wordcount.MainClass &lt;input path&gt; &lt;output path&gt;
[root@node1 ~]# yarn jar wc.jar com.bjsxt.mr.wordcount.MainClass &#x2F;input &#x2F;output2


yarn   jar   [&#x2F;path&#x2F;to&#x2F;your&#x2F;]jar.jar package.MainClass  &#x2F;&lt;inputpath&gt;  &#x2F;&lt;outputpath&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h4 id="1-9-二次排序（先了解）"><a href="#1-9-二次排序（先了解）" class="headerlink" title="1.9    二次排序（先了解）"></a>1.9    二次排序（先了解）</h4><p>在map阶段按照key对键值对进行排序，对值不排序。如果相对value进行排序，就需要二次排序。</p>
<p>需求：查找每年的最高气温<br>数据格式：年份为key，每天的气温是value</p>
<p>所谓二次排序：<br>1、新的key应该是输入的key和value的组合<br>2、按照复合key进行比较排序<br>3、分区比较器和分组比较器只对复合key中的原生key进行分区和分组<br><a target="_blank" rel="noopener" href="https://ibb.co/642yWvd">流程图</a></p>
<h3 id="4-mapreduce计算流程"><a href="#4-mapreduce计算流程" class="headerlink" title="4    mapreduce计算流程"></a>4    mapreduce计算流程</h3><h4 id="1-1-mapreduce计算流程"><a href="#1-1-mapreduce计算流程" class="headerlink" title="1.1    mapreduce计算流程"></a>1.1    mapreduce计算流程</h4><p>首先将block块进行逻辑切片的计算，每个切片（split）对应一个map任务<br>切片是为了将block数量和map任务数量解耦。<br>map读取切片数据，默认按行读取，作为键值对交给map方法，其中key是当前读取的行在文件中的字节偏移量，value就是读取的当前行的内容。<br>map开始计算，自定义的逻辑。<br>map将输出的kv首先写到环形缓冲区，在写之前计算分区号（默认按照key的hash值对reducer的个数取模）。<br>环形缓冲区默认100MB，阈值80%，如果写入的kv对达到了80%则发生溢写，溢写的时候要先对键值对按照分区号进行分区，相同分区按照key的字典序排序，溢写到磁盘。如果溢写的文件数量达到了三个，则发生map端归并操作，此时如果指定了combiner，则按照combiner合并数据。<br>当一个map任务完成之后，所有的reducertask向其发送http get请求，下载它们所属的分区数据。此过程称为shuffle，洗牌。<br>当所有map任务运行结束，开始reduce任务</p>
<p>在reduce开始之前，根据设定的归并因子，进行多伦的归并操作，非最后一轮的归并的结果文件被存入到硬盘上，最后一轮归并的结果直接传递给reduce，reduce迭代计算。<br>reduce计算结束后将结果写到HDFS文件中，每一个reducer task任务都会在作业输出路径下产生一个结果文件part-r-00xxx。同时执行成功时会产生一个空的_SUCCESS文件，该文件是一个标识文件。MR1-&gt;MR2-&gt;MR3</p>
<h4 id="1-2-作业提交流程"><a href="#1-2-作业提交流程" class="headerlink" title="1.2    作业提交流程"></a>1.2    作业提交流程</h4><p>1、    客户端向RM取号(获取作业的ID)<br>2、    客户端检查作业输入输出（如果输入路径不存在则抛出异常；如果输出路径存在也抛出异常），计算切片，解析配置信息<br>3、    客户端将jar包、配置信息以及切片信息上传到HDFS<br>4、    客户端向RM发送提交作业的请求<br>5、    RM调度一个NM，在NM上的一个容器中运行MRAppMaster，一个作业对应一个MRAppMaster。<br>6、    MRAppMaster首先获取HDFS中的作业信息，计算出当前作业需要多少个map任务，多少个reduce任务<br>7、    MRAppMaster（AM）向RM为map任务申请容器，AM跟NM通信把容器启动起来，运行map任务，容器中的YARNChild会首先本地化conf、切片信息以及jar包<br>8、    当map任务完成达到5%的时候，AM向RM为reduce任务申请容器<br>9、    当MR中最后一个任务运行结束，AM向客户端发送作业完成信息。MR的中间数据销毁，容器销毁，计算结果存档到历史服务器</p>
<h4 id="2-目标"><a href="#2-目标" class="headerlink" title="2    目标"></a>2    目标</h4><p>客户端作业提交源码分析<br>框架：MapTask<br>框架：ReduceTask</p>
<h4 id="2-1-客户端作业提交源码分析"><a href="#2-1-客户端作业提交源码分析" class="headerlink" title="2.1    客户端作业提交源码分析"></a>2.1    客户端作业提交源码分析</h4><p><a target="_blank" rel="noopener" href="https://ibb.co/0n0ww1m">教程</a><br>Mapper类的map方法中添加：</p>
<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token class-name">Thread</span><span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">99999999</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">MainClass</span>类中注释掉：
<span class="token comment">//configuration.set("mapreduce.framework.name","local");</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>重新打jar包，发布到服务器上运行：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">yarn jar mywc.jar com.bjsxt.mr.wc.MainClass &lt;inputPath&gt; &lt;outputPath&gt;

[root@node1 ~]# hdfs dfs -ls -R &#x2F;tmp&#x2F;hadoop-yarn&#x2F;
[root@node1 ~]# hdfs dfs -get &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;root&#x2F;.staging&#x2F;job_1582529707113_0005<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>查看job.xml（Ctrl+Alt+L格式化代码）<br>配置信息默认值一部分来自于MRJobConfig接口<br>一部分来自*-default.xml文件<br>一部分来自用户的设置：程序设置和xxx-site.xml设置<br>Configuration<br>xx-default.xml<br>xx-site.xml<br>MRJobConfig<br>观察默认值<br>JobContextImpl<br>JobConf conf<br>观察默认值<br>Job<br>set*()<br>waitForCompletion(true)</p>
<p>MainClass中使用<br>//提交作业，等待作业完成<br>job.waitForCompletion(true)</p>
<p>提交作业:<br>提交作业属于异步操作，当执行submit()方法之后，由于job.waitForCompletion(true)参数为true，所以会执行monitorAndPrintJob()方法不断打印输出执行的过程。</p>
<p>…. 略》》》 </p>
<h4 id="2-2-框架：MapTask"><a href="#2-2-框架：MapTask" class="headerlink" title="2.2    框架：MapTask"></a>2.2    框架：MapTask</h4><p>从切片读取数据，给map方法，map方法输出的键值对写到环形缓冲区<br>在写到环形缓冲区之前计算分区号<br>环形缓冲区排序后溢写到map端本地磁盘，可能会有合并的过程，3<br>可以设置环形缓冲区大小和阈值<br>排序所使用的算法：默认是快排，可以设置</p>
<p>可以自定义key和value<br>排序比较器可以自定义</p>
<p>可以设置Combiner</p>
<h5 id="2-2-1-map输入input"><a href="#2-2-1-map输入input" class="headerlink" title="2.2.1    map输入input"></a>2.2.1    map输入input</h5><h6 id="2-2-1-1-源码从哪里开始看？"><a href="#2-2-1-1-源码从哪里开始看？" class="headerlink" title="2.2.1.1    源码从哪里开始看？"></a>2.2.1.1    源码从哪里开始看？</h6><p><a target="_blank" rel="noopener" href="https://ibb.co/xLZpjsx">流程</a></p>
<p>从YarnChild开始<br>在idea中通过alt+ctrl+&lt;-或-&gt;后退或前进<br>Ctrl+Shift+N -&gt;输入YarnChild-&gt;Ctrl+G (定位到哪一行)-&gt;输入163：1<br>YarnChild</p>
<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">final</span> <span class="token class-name">Task</span> taskFinal <span class="token operator">=</span> <span class="token class-name">Task</span><span class="token punctuation">;</span>
childUGI<span class="token punctuation">.</span><span class="token function">doAs</span><span class="token punctuation">(</span><span class="token class-name">PrivilegedExceptAction</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">)</span>→<span class="token punctuation">&#123;</span>

	<span class="token class-name">FileSystem</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>job<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setWorkingDirectory</span><span class="token punctuation">(</span>job<span class="token punctuation">.</span><span class="token function">getWorkingDirectory</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	taskFinal<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span>umbiilical<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//run the task</span>
	<span class="token keyword">return</span> <span class="token keyword">null</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>光标点到run()方法名称上，Ctrl+Alt+B-&gt;选择MapTask类</p>
<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">abstract</span> <span class="token keyword">void</span> <span class="token function">run</span><span class="token punctuation">(</span><span class="token class-name">JobConf</span> job<span class="token punctuation">,</span><span class="token class-name">TaskUmbilicalProtocal</span> umnilical<span class="token punctuation">)</span>
	<span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span><span class="token class-name">ClassNotFoundException</span> <span class="token punctuation">,</span><span class="token class-name">InterrupteException</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>MapTask</p>
<p>MapTask实现了run方法：</p>
<h6 id=""><a href="#" class="headerlink" title=""></a></h6><p>2.2.1.2    runNewMapper真正执行Map任务(Hadoop2.x+)</p>
<hr>
<p>待续</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">tangyuan</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://tangyuan59.github.io/2021/12/22/hadoop-ji-chu/">http://tangyuan59.github.io/2021/12/22/hadoop-ji-chu/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">tangyuan</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Hadoop/">
                                    <span class="chip bg-color">Hadoop</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2021/12/22/hadoop-ji-chu/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/5.jpg" class="responsive-img" alt="hadoop基础">
                        
                        <span class="card-title">hadoop基础</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-12-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                    大数据
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Hadoop/">
                        <span class="chip bg-color">Hadoop</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/12/21/zookeeper/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/12.jpg" class="responsive-img" alt="ZooKeeper">
                        
                        <span class="card-title">ZooKeeper</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-12-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                    大数据
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/ZooKeeper/">
                        <span class="chip bg-color">ZooKeeper</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2021</span>
            
            <a href="/about" target="_blank">tangyuan</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">34.8k</span>
            
            
            
            
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link ">
    <a href="https://github.com/tangyuan59/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1010337861@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1010337861" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1010337861" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
